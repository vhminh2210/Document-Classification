{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unsupervised Word2Vec with Negative Sampling\n\nPyTorch implementation of Negative Sampling for Word2Vec introduced in **Distributed Representations of Words and Phrases and their Compositionality (NIPS 2013)**\n\n# Table of contents\n* [Preamble](#Preamble)\n* [Data preparation](#Data-preparation)\n    * [Load raw data](#Load-raw-data)\n    * [Train-test split](#Train-test-split)\n    * [Setup vocabulary utils](#Setup-vocabulary-utils)\n* [PyTorch Dataset class](#PyTorch-Dataset-class)\n* [Word2Vec Module](#Word2Vec-Module)\n    * [Embedding module](#Embedding-module)\n    * [Sampling utilities](#Sampling-utilities)\n* [Training](#Training)","metadata":{}},{"cell_type":"markdown","source":"## Preamble","metadata":{}},{"cell_type":"code","source":"# Preamble\nimport time, random\nimport os, sys\nimport math\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 64","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\nThe word embeddings is trained on YELP-review dataset: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset","metadata":{}},{"cell_type":"markdown","source":"## Load raw data","metadata":{}},{"cell_type":"code","source":"# CSV Preparation\ndata_file = open(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\")\ndata = []\n\ncnt = 1569264 # Size of YELP 2015 dataset\n# cnt = 10000\n\nfor line in data_file:\n    data.append(json.loads(line))\n    cnt -= 1\n    if cnt == 0:\n        break\n    \ndata_file.close()\ndf = pd.DataFrame(data)\n\nprint(\"Number of datapoints:\", len(df))\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-test split","metadata":{}},{"cell_type":"code","source":"df_size = len(df)\nidx = [x for x in range(df_size)]\nrandom.Random(555).shuffle(idx)\n\ntrain_num = int(df_size * 0.8)\nval_num = int(df_size * 0.1)\ntest_num = int(df_size * 0.1)\n\n# print(train_num, val_num, test_num)\n\ntrain_idx = idx[:train_num]\nval_idx = idx[train_num : (train_num + val_num)]\ntest_idx = idx[(train_num + val_num) : ]\n\ntrain_df = df.iloc[train_idx]\nval_df = df.iloc[val_idx]\ntest_df = df.iloc[test_idx]\n\nprint('Size of trainset:', len(train_df))\nprint('Size of valset:', len(val_df))\nprint('Size of testset:', len(test_df))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup vocabulary utils\nHere the frequency counter, vocab object and tokenizer is defined.\n\nReference: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html","metadata":{}},{"cell_type":"code","source":"# Set up Vocab\n# Source: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import vocab, build_vocab_from_iterator\n\nfrom collections import Counter\n\ntokenizer = get_tokenizer(\"basic_english\")\n            \ndef get_counter(df_lists):\n    counter = Counter()\n    for _df in df_lists:\n        for i in range(len(_df)):\n            counter.update(tokenizer(_df.iloc[i]['text']))\n    return counter\n\n# counter = get_counter([train_df, val_df]) - train_df is too large already.\ncounter = get_counter([train_df])\n\nvocab = vocab(\n    counter,\n    specials= [\"<unk>\"],\n    min_freq= 6\n)\nvocab.set_default_index(vocab[\"<unk>\"])\n\nvocab_size = len(vocab)\nprint('Vocabulary size:', vocab_size)\n\ntorch.save(vocab, 'vocab.pth')\ntorch.save(counter, 'counter.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Dataset class\nIn this section, the `YELPDataset` class is defined with zero-padding batch function `collate_batch`.","metadata":{}},{"cell_type":"code","source":"# YELP Dataset\ndef collate_batch(batch):\n    '''\n    Collate batch with zero-padding\n    Consult: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n    '''\n    texts = []\n    labels = []\n    mask = []\n    for _text, _label in batch:\n        texts.append(_text)\n        labels.append(_label)\n        \n    L = max([len(text) for text in texts])\n    \n    for i in range(len(texts)):\n        l = texts[i].shape[0]\n        cur_mask = torch.ones(L)\n        if l < L:\n            cur_mask[l:L] = 0\n            # Zero-padding text, only on one side.\n            texts[i] = F.pad(texts[i], (0, L-l), 'constant', 0)\n            \n        mask.append(cur_mask)\n    \n    texts = torch.stack(texts)\n    labels = torch.stack(labels)\n    mask = torch.stack(mask)\n    return texts, labels, mask\n\nclass YELPDataset(Dataset):\n    def __init__(self, df, vocab, tokenizer, df_sort= True):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n        self.len_vocab = len(vocab)\n        if df_sort:\n            self.sort_df_by_txt_len()\n    \n    def sort_df_by_txt_len(self):\n        len_list = [-len(self.df.iloc[i]['text']) for i in range(len(self.df))]\n        self.df = self.df.iloc[np.argsort(len_list)]\n    \n    def __len__(self):\n        return len(self.df)\n\n    def text_pipeline(self, x):\n        return self.vocab(self.tokenizer(x))\n\n    def label_pipeline(self, x):\n        return int(x) - 1\n    \n    def __getitem__(self, idx):\n        txt = self.text_pipeline(self.df.iloc[idx]['text'])\n        txt = torch.tensor(txt, dtype= torch.int64)\n#         txt = F.one_hot(txt, num_classes= self.len_vocab)\n        \n        label = self.label_pipeline(self.df.iloc[idx]['stars'])\n        label = torch.tensor(label, dtype= torch.int64)\n        \n        return (txt, label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset, Dataloader\ntrainset = YELPDataset(train_df, vocab, tokenizer)\nvalset = YELPDataset(val_df, vocab, tokenizer)\ntestset = YELPDataset(test_df, vocab, tokenizer)\n\ntrainloader = DataLoader(trainset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch)\nvalloader = DataLoader(valset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch)\ntestloader = DataLoader(testset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch)\n\nfor X, y, mask in trainloader:\n    print(\"Shape of Texts:\", X.shape)\n    print(\"Shape of Labels:\", y.shape)\n    print(\"Shape of Mask:\", mask.shape)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec Module","metadata":{}},{"cell_type":"code","source":"# Task configs\nVOCAB_SIZE = len(vocab)\nEMBEDDING_SIZE = 200\nCONTEXT_WINDOW = 10\nNUM_SAMPLES = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding module","metadata":{}},{"cell_type":"code","source":"# Word2vec embedding module\nclass NegSamplingEmbedding(nn.Module):\n    '''\n    Vocab_size: V\n    Embedding_size: E\n    Text_length: L\n    Batch_size: B\n    \n    Consult: https://github.com/mindspore-courses/DeepNLP-models-MindSpore/\n            blob/main/notebooks/02.Skip-gram-Negative-Sampling.ipynb\n    '''\n    def __init__(self, vocab_size, embedding_size):\n        super(NegSamplingEmbedding, self).__init__()\n        self.U = nn.Embedding(vocab_size, embedding_size) # Center embedding\n        self.V = nn.Embedding(vocab_size, embedding_size) # Outside embedding\n        self.LogSig = nn.LogSigmoid()\n        \n    def forward(self, wc, wo, wk, mask_c, mask_o, check_shape= False):\n        vc = self.V(wc) # Center embedding. Shape: (B, L, E)\n        uo = self.U(wo) # Outside embedding. Shape: (B, L, C, E)\n        uk = self.U(wk) # Random embedding. Shape: (B, L, C, E, K)\n        \n        if check_shape:\n            B = uk.shape[0]\n            L = uk.shape[1]\n            C = uk.shape[2]\n            K = uk.shape[3]\n            E = uk.shape[4]\n            print(f\"Basic shapes: B = {B}; L = {L}; C = {C}; K = {K}; E = {E}\")\n            print('*********************************')\n            print('Shape of vc:', vc.shape)\n            print('Shape of uo:', uo.shape)\n            print('Shape of uk:', uk.shape)\n            print('*********************************')\n        cmp1 = torch.einsum('blce,ble->blc', uo, vc) # Shape: (B, L, C)\n        cmp2 = torch.einsum('blcke,ble->blck', uk, vc) # Shape: (B, L, C, K)\n        \n        cmp1 = self.LogSig(cmp1) * mask_o # Shape: (B, L, C)\n        cmp2 = self.LogSig(-cmp2) # Shape: (B, L, C, K)\n        cmp2 = torch.einsum('blck->blc', cmp2) * mask_o # Shape: (B, L, C)\n    \n        cmp1 = torch.einsum('blc->bl', cmp1) # Shape: (B, L)\n        cmp2 = torch.einsum('blc->bl', cmp2) # Shape: (B, L)\n        \n        loss = torch.mean(cmp1 + cmp2)\n        \n        if check_shape:\n            print('Shape of cmp1:', cmp1.shape)\n            print('Shape of cmp2:', cmp2.shape)\n            print('Shape of LOSS:', loss.shape)\n        return -loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sampling utilities","metadata":{}},{"cell_type":"code","source":"# Negative Sampling\ndef NoiseDistribution(counter, vocab, Z= 0.001):\n    '''\n    Noise distribution P_n(w) = [U(w)^0.75]/Z .\n    '''\n    Fr = np.zeros(len(vocab)) # Frequency\n    for k in counter.keys():\n        ix = vocab[k]\n        Fr[ix] += counter[k]\n    \n    U = Fr / np.sum(Fr)\n    P = (U ** 0.75) / Z\n    return Fr, P\n\ndef SubSamplingMask(Fr, t= 1e-5):\n    '''\n    Subsampling mask\n    '''\n    masks = np.ones(Fr.shape[0])\n    for i in range(Fr.shape[0]):\n        break\n        coin = np.random.binomial(1, np.sqrt(t/Fr[i]))\n        masks[i] = coin\n        \n    return masks\n\ndef SamplingDistribution(vocab, P):\n    '''\n    Construct sampling vocabulary\n    Consult: https://github.com/mindspore-courses/DeepNLP-models-MindSpore/\n            blob/main/notebooks/02.Skip-gram-Negative-Sampling.ipynb\n    '''\n    unigram_table = []\n    cnt = 0\n    vocab_size = len(vocab)\n    for i in range(vocab_size):\n        unigram_table = unigram_table + ([i] * int(P[i]))\n    return np.array(unigram_table)\n\ndef NegativeSampling(unigram_table, k):\n    '''\n    Negative sampling over unigram_table, taking k random samples.\n    '''\n    idx = np.random.choice(unigram_table.shape[0], k)\n    return torch.tensor(unigram_table[idx])\n\ndef OutsideSampling(text, C, K, vocab, Fr= None, P= None, unigram_table= None):\n    '''\n    Construction of wo, mask_o.\n    <text> is expected to have type of torch.Tensor with shape of (L,)\n    \n    Unigram frequency: Fr\n    Noise distribution: P\n    \n    Text length: L\n    Context window: C\n    Number of random samples: K\n    Embedding size: E\n    '''\n    assert C % 2 == 0 # Context windows must be divisible by 2 \n    hC = int(C/2) # Half window\n    \n    wo = [] # Shape: (L, C)\n    mask_o = [] # Shape: (L, C)\n    wk = [] # Shape: (L, C, K)\n    \n    L = text.shape[0]\n    txt = F.pad(text, (hC, hC), 'constant', 0) # [<unk>] padding on both sides\n    \n    for i in range(L):\n        l = i\n        r = i + C\n        mid = i + hC\n        cur_mask_o = torch.ones(C)\n        \n        if i-hC < 0:\n            cur_mask_o[0 : (hC-i)] = 1\n        if i+hC > L-1:\n            cur_mask_o[(L-i-1+hC) : C] = 1\n\n        wo.append(torch.cat((txt[l:mid], txt[(mid+1):(r+1)]), dim= 0))\n        mask_o.append(cur_mask_o)\n        \n    wo = torch.stack(wo)\n    mask_o = torch.stack(mask_o)\n    \n    return wo, mask_o\n\ndef BatchSampling(texts, C, K, vocab, Fr= None, P= None, unigram_table= None):\n    '''\n    Construction of wc, wo, wk, mask_c, mask_o.\n    <texts> is expected to have type of list(torch.Tensor) with shape (B, L)\n    \n    Unigram frequency: Fr\n    Noise distribution: P\n    \n    Batch size: B\n    Text length: L\n    Context window: C\n    Number of random samples: K\n    Embedding size: E\n    '''\n    wc = texts # Shape: (B, L)\n    wo = [] # Shape: (B, L, C)\n    wk = [] # Shape: (B, L, C, K)\n    \n    mask_o = [] # Shape: (B, L, C)\n    \n    B = texts.shape[0]\n    L = texts.shape[1]\n        \n    # Get outside-tokens with zero-padding\n    for i in range(B):\n        cur_wo, cur_mask_o = OutsideSampling(texts[i], C, K, vocab, \n                                              Fr, P, unigram_table)\n        wo.append(cur_wo)\n        mask_o.append(cur_mask_o)\n        \n    # Batch-level subsampling\n    if unigram_table is None:\n        assert P is not None and Fr is not None\n        masks = SubSamplingMask(Fr)\n        unigram_table = SamplingDistribution(vocab, P*masks)\n    \n    # Random sampling over unigram_table with shape of (B, L, C, K)\n    idx = torch.randint(low= 0, high= unigram_table.shape[0], size= (B, L, C, K))\n    wk = torch.tensor(unigram_table[idx])\n        \n    # Tensorize\n    wo = torch.stack(wo)\n    mask_o = torch.stack(mask_o)\n    \n    return wc, wo, wk, mask_o","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Module testing","metadata":{}},{"cell_type":"code","source":"# Defining model, noise distribution\nFr, P = NoiseDistribution(counter, vocab)\nmodel = NegSamplingEmbedding(VOCAB_SIZE, EMBEDDING_SIZE).to(DEVICE)\n\nmasks = SubSamplingMask(Fr)\nunigram_table = SamplingDistribution(vocab, P*masks)\n\n# Dimension checking\nfor X, y, mask_c in trainloader:\n    print('Shape of X:', X.shape)\n    print('Shape of mask:', mask_c.shape)\n    X, y, mask_c = X.to(DEVICE), y.to(DEVICE), mask_c.to(DEVICE)\n    wc, wo, wk, mask_o = BatchSampling(X, \n                                       C= CONTEXT_WINDOW,\n                                       K= NUM_SAMPLES,\n                                       vocab= vocab,\n                                       unigram_table= unigram_table)\n    wc, wo, wk = wc.to(DEVICE), wo.to(DEVICE), wk.to(DEVICE)\n    mask_o = mask_o.to(DEVICE)\n    print('Shape of wc:', wc.shape)\n    print('Shape of wo:', wo.shape)\n    print('Shape of wk:', wk.shape)\n    print('*********************************')\n    loss = model(wc, wo, wk, mask_c, mask_o, check_shape= True)\n    print('*********************************')\n    print('LOSS =', loss)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Traing configs\nLR = 1e-3\nEPOCHS = 1\nITER = EPOCHS * len(trainloader)\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr= LR)\nSCHEDULER = lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = ITER)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train procedures\ndef train(trainloader, model, optimizer, scheduler, vocab, unigram_table):\n    model.train()\n    tloss = []\n    cur_loss = 1e9\n    for i, (X, y, mask_c) in enumerate(trainloader):\n        X, y, mask_c = X.to(DEVICE), y.to(DEVICE), mask_c.to(DEVICE)\n        wc, wo, wk, mask_o = BatchSampling(X, \n                                   C= CONTEXT_WINDOW,\n                                   K= NUM_SAMPLES,\n                                   vocab= vocab,\n                                   unigram_table= unigram_table)\n        wc, wo, wk = wc.to(DEVICE), wo.to(DEVICE), wk.to(DEVICE)\n        mask_o = mask_o.to(DEVICE)\n        \n        loss = model(wc, wo, wk, mask_c, mask_o)\n        \n        if i % 1000 == 0:\n            tloss.append(loss.cpu().detach().numpy())\n            print(f'Iter {i}, loss = ',tloss[-1])\n            if tloss[-1] < cur_loss:\n                cur_loss = tloss[-1]\n                print('Saving model...')\n                torch.save(model.state_dict(), 'word2vec.pth')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n    if tloss[-1] < cur_loss:\n        cur_loss = tloss[-1]\n        torch.save(model.state_dict(), 'word2vec.pth')\n    return tloss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING\niter_loss = []\nepoch_loss = []\n\nfor t in range(EPOCHS):\n    print(f'Epoch {t} starts.')\n    # Epoch-level subsampling\n    Fr, P = NoiseDistribution(counter, vocab)\n    masks = SubSamplingMask(Fr)\n    unigram_table = SamplingDistribution(vocab, P*masks)\n    \n    tloss = train(trainloader, model, OPTIMIZER, SCHEDULER, vocab, unigram_table)\n    \n    iter_loss = iter_loss + tloss\n    epoch_loss.append(sum(tloss) / len(tloss))\n    \n    print(f'Epoch {t}: LOSS = {epoch_loss[-1]}')\n    \nfig, axes = plt.subplots()\naxes.plot(iter_loss, label = 'train-loss')\naxes.legend()\naxes.set_xlabel('Iteration')\naxes.set_ylabel('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}