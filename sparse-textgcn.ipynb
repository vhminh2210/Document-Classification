{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100},{"sourceId":8053014,"sourceType":"datasetVersion","datasetId":4741673},{"sourceId":8528311,"sourceType":"datasetVersion","datasetId":5093015},{"sourceId":8556493,"sourceType":"datasetVersion","datasetId":5107660}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GCN for Text Classification\nPyTorch reimplementation of **Graph Convolutional Networks for Text Classification (AAAI 2019)**\n\n# Table of Content\n* [Preamble](#Preamble)\n* [Data preparation](#Data-preparation)\n    * [Load raw data](#Load-raw-data)\n    * [Train-test split](#Train-test-split)\n    * [Setup vocabulary utils](#Setup-vocabulary-utils)\n    * [Load pretrained Embeddings](#Load-pretrained-Embeddings)\n        * [Word2Vec Module](#Word2Vec-Module)\n    * [Setup statistics utils](#Setup-statistics-utils)\n    * [Build Text graph](#Build-Text-graph)\n* [PyTorch Dataset class](#PyTorch-Dataset-class)\n* [TextGCN Module](#TextGCN-Module)\n* [Training](#Training)\n    * [Prepare text graphs](#Prepare-text-graphs)\n    * [Training configs](#Training-configs)\n    * [Verify training accuracy](#Verify-training-accuracy)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T03:38:16.134645Z","iopub.execute_input":"2024-04-10T03:38:16.135238Z","iopub.status.idle":"2024-04-10T03:38:24.016851Z","shell.execute_reply.started":"2024-04-10T03:38:16.135211Z","shell.execute_reply":"2024-04-10T03:38:24.015998Z"}}},{"cell_type":"markdown","source":"# Preamble","metadata":{}},{"cell_type":"code","source":"# Preamble\nimport time, random\nimport re, string\nimport os, sys\nimport math\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nCPU = torch.device(\"cpu\")\nBATCH_SIZE = 65536","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:35.165925Z","iopub.execute_input":"2024-05-30T11:34:35.166325Z","iopub.status.idle":"2024-05-30T11:34:35.174691Z","shell.execute_reply.started":"2024-05-30T11:34:35.166294Z","shell.execute_reply":"2024-05-30T11:34:35.173460Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\nThe word embeddings is trained on YELP-review dataset: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset","metadata":{}},{"cell_type":"markdown","source":"## Load raw data","metadata":{}},{"cell_type":"code","source":"# # CSV Preparation\n# data_file = open(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\")\n# data = []\n\n# # cnt = 1569264 # Size of YELP 2015 dataset\n# cnt = 65536\n\n# for line in data_file:\n#     data.append(json.loads(line))\n#     cnt -= 1\n#     if cnt == 0:\n#         break\n    \n# data_file.close()\n# df = pd.DataFrame(data)\n\n# print(\"Number of datapoints:\", len(df))\n# df.head()\n\ntrain_ds = pd.read_csv('/kaggle/input/smolcsv/r52-train-stemmed.csv')\nval_ds = pd.read_csv('/kaggle/input/smolcsv/r52-dev-stemmed.csv')\ntest_ds = pd.read_csv('/kaggle/input/smolcsv/r52-test-stemmed.csv')\n\ntrain_ds.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:35.180043Z","iopub.execute_input":"2024-05-30T11:34:35.180412Z","iopub.status.idle":"2024-05-30T11:34:35.346082Z","shell.execute_reply.started":"2024-05-30T11:34:35.180366Z","shell.execute_reply":"2024-05-30T11:34:35.344834Z"},"trusted":true},"execution_count":192,"outputs":[{"execution_count":192,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  bahia cocoa review shower continu week bahia c...   \n1  champion product approv stock split champion p...   \n2  comput termin system cpml complet sale comput ...   \n3  cobanco inc cbco year net shr ct dlr net asset...   \n4  intern inc qtr jan oper shr loss two ct profit...   \n\n                                                edge intent  intent_label  \\\n0  bahia cocoa review shower continu week bahia c...  cocoa             1   \n1  champion product approv stock split champion p...   earn             2   \n2  comput termin system cpml complet sale comput ...    acq             3   \n3  cobanco inc cbco year net shr ct dlr net asset...   earn             2   \n4  intern inc qtr jan oper shr loss two ct profit...   earn             2   \n\n   label  \n0      0  \n1      1  \n2      2  \n3      1  \n4      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>edge</th>\n      <th>intent</th>\n      <th>intent_label</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bahia cocoa review shower continu week bahia c...</td>\n      <td>bahia cocoa review shower continu week bahia c...</td>\n      <td>cocoa</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>champion product approv stock split champion p...</td>\n      <td>champion product approv stock split champion p...</td>\n      <td>earn</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>comput termin system cpml complet sale comput ...</td>\n      <td>comput termin system cpml complet sale comput ...</td>\n      <td>acq</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n      <td>cobanco inc cbco year net shr ct dlr net asset...</td>\n      <td>earn</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n      <td>intern inc qtr jan oper shr loss two ct profit...</td>\n      <td>earn</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train-test split\nOnly trainset is needed for building pretrained **TextGCN**.","metadata":{}},{"cell_type":"code","source":"# df_size = len(df)\n# idx = [x for x in range(df_size)]\n# random.Random(555).shuffle(idx)\n\n# train_num = int(df_size)\n\n# train_idx = idx[:train_num]\n\n# train_df = df.iloc[train_idx]\n\nprint('Size of trainset:', len(train_ds))\nprint('Size of valset:', len(val_ds))\nprint('Size of testset:', len(test_ds))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:35.348207Z","iopub.execute_input":"2024-05-30T11:34:35.348590Z","iopub.status.idle":"2024-05-30T11:34:35.356485Z","shell.execute_reply.started":"2024-05-30T11:34:35.348558Z","shell.execute_reply":"2024-05-30T11:34:35.355242Z"},"trusted":true},"execution_count":193,"outputs":[{"name":"stdout","text":"Size of trainset: 5879\nSize of valset: 2568\nSize of testset: 2568\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup vocabulary utils\nHere the frequency counter, vocab object and tokenizer is defined.\n\nReference: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html","metadata":{}},{"cell_type":"code","source":"# Set up Vocab\n# Source: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import vocab, build_vocab_from_iterator\n\nfrom collections import Counter\n\ntokenizer = get_tokenizer(\"basic_english\")\n\ndef get_counter(texts):\n    counter = Counter()\n    for text in texts:\n        counter.update(tokenizer(text))\n    return counter\n\ndef get_vocab(texts):\n    counter = get_counter(texts)\n    vocabulary = vocab(\n        counter,\n        specials= [\"<unk>\"],\n        min_freq= 1\n    )\n    vocabulary.set_default_index(vocabulary[\"<unk>\"])\n    return vocabulary\n\ndef text_pipeline(text, vocabulary):\n    return vocabulary(tokenizer(text))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:35.358338Z","iopub.execute_input":"2024-05-30T11:34:35.358767Z","iopub.status.idle":"2024-05-30T11:34:35.369794Z","shell.execute_reply.started":"2024-05-30T11:34:35.358738Z","shell.execute_reply":"2024-05-30T11:34:35.368592Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"markdown","source":"## Load pretrained Embeddings\nHere the pretrained embeddings along with the grand vocabulary is prepared. However, pretrained high-level feature representation (or embeddings) does not work well with **TextGCN**. The reason seems to stem from $G$ where edge weights are based on **discrete statistical quantities** ($TF-IDF$ and $PMI$) while word embeddings assume a **continuous space** for word representation and interaction (via **continuous** function, for example, **cosine similarity**)","metadata":{}},{"cell_type":"code","source":"# Prepare vocab, counter\ngrand_vocab = torch.load('/kaggle/input/hlt-word2vec/vocab.pth')\ncounter = torch.load('/kaggle/input/hlt-word2vec/counter.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:35.373532Z","iopub.execute_input":"2024-05-30T11:34:35.374042Z","iopub.status.idle":"2024-05-30T11:34:36.267446Z","shell.execute_reply.started":"2024-05-30T11:34:35.374010Z","shell.execute_reply":"2024-05-30T11:34:36.265944Z"},"trusted":true},"execution_count":195,"outputs":[]},{"cell_type":"markdown","source":"### Word2Vec Module\nThe Word2Vec Negative Sampling module is taken from: https://www.kaggle.com/code/vhminh2210/negsampling-exps","metadata":{}},{"cell_type":"code","source":"VOCAB_SIZE = len(grand_vocab)\nINIT_EMBEDDING_DIM = 200","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.268772Z","iopub.execute_input":"2024-05-30T11:34:36.269151Z","iopub.status.idle":"2024-05-30T11:34:36.274799Z","shell.execute_reply.started":"2024-05-30T11:34:36.269118Z","shell.execute_reply":"2024-05-30T11:34:36.273453Z"},"trusted":true},"execution_count":196,"outputs":[]},{"cell_type":"code","source":"# Negative sampling embedding module\nclass NegSamplingEmbedding(nn.Module):\n    '''\n    Vocab_size: V\n    Embedding_size: E\n    Text_length: L\n    Batch_size: B\n    \n    Consult: https://github.com/mindspore-courses/DeepNLP-models-MindSpore/\n            blob/main/notebooks/02.Skip-gram-Negative-Sampling.ipynb\n    '''\n    def __init__(self, vocab_size, embedding_size):\n        super(NegSamplingEmbedding, self).__init__()\n        self.U = nn.Embedding(vocab_size, embedding_size) # Center embedding\n        self.V = nn.Embedding(vocab_size, embedding_size) # Outside embedding\n        self.LogSig = nn.LogSigmoid()\n        \n    def forward(self, wc, wo, wk, mask_c, mask_o, check_shape= False):\n        vc = self.V(wc) # Center embedding. Shape: (B, L, E)\n        uo = self.U(wo) # Outside embedding. Shape: (B, L, C, E)\n        uk = self.U(wk) # Random embedding. Shape: (B, L, C, E, K)\n        \n        if check_shape:\n            B = uk.shape[0]\n            L = uk.shape[1]\n            C = uk.shape[2]\n            K = uk.shape[3]\n            E = uk.shape[4]\n            print(f\"Basic shapes: B = {B}; L = {L}; C = {C}; K = {K}; E = {E}\")\n            print('*********************************')\n            print('Shape of vc:', vc.shape)\n            print('Shape of uo:', uo.shape)\n            print('Shape of uk:', uk.shape)\n            print('*********************************')\n        cmp1 = torch.einsum('blce,ble->blc', uo, vc) # Shape: (B, L, C)\n        cmp2 = torch.einsum('blcke,ble->blck', uk, vc) # Shape: (B, L, C, K)\n        \n        cmp1 = self.LogSig(cmp1) * mask_o # Shape: (B, L, C)\n        cmp2 = self.LogSig(-cmp2) # Shape: (B, L, C, K)\n        cmp2 = torch.einsum('blck->blc', cmp2) * mask_o # Shape: (B, L, C)\n    \n        cmp1 = torch.einsum('blc->bl', cmp1) # Shape: (B, L)\n        cmp2 = torch.einsum('blc->bl', cmp2) # Shape: (B, L)\n        \n        loss = torch.mean(cmp1 + cmp2)\n        \n        if check_shape:\n            print('Shape of cmp1:', cmp1.shape)\n            print('Shape of cmp2:', cmp2.shape)\n            print('Shape of LOSS:', loss.shape)\n        return -loss","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.276537Z","iopub.execute_input":"2024-05-30T11:34:36.276991Z","iopub.status.idle":"2024-05-30T11:34:36.299915Z","shell.execute_reply.started":"2024-05-30T11:34:36.276948Z","shell.execute_reply":"2024-05-30T11:34:36.298078Z"},"trusted":true},"execution_count":197,"outputs":[]},{"cell_type":"code","source":"# Prepare word embeddings\nWORD2VEC_PATH = '/kaggle/input/hlt-word2vec/word2vec.pth'\nword2vec = NegSamplingEmbedding(VOCAB_SIZE, INIT_EMBEDDING_DIM)\nword2vec.load_state_dict(torch.load(WORD2VEC_PATH, map_location= DEVICE))\nword2vec.eval()\n\nW_e = word2vec.V.weight.detach().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.301479Z","iopub.execute_input":"2024-05-30T11:34:36.301902Z","iopub.status.idle":"2024-05-30T11:34:36.763684Z","shell.execute_reply.started":"2024-05-30T11:34:36.301864Z","shell.execute_reply":"2024-05-30T11:34:36.762402Z"},"trusted":true},"execution_count":198,"outputs":[]},{"cell_type":"markdown","source":"## Setup statistics utils\nHere the statistics metrics $TF-IDF$ and $PMI$ are defined. Words in Vocab are indexed from $1\\dots N$ and documents are indexed from $1\\dots V$","metadata":{}},{"cell_type":"code","source":"import scipy.sparse as sp # Sparse matrix utilities\nfrom collections import defaultdict ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.765054Z","iopub.execute_input":"2024-05-30T11:34:36.765519Z","iopub.status.idle":"2024-05-30T11:34:36.770948Z","shell.execute_reply.started":"2024-05-30T11:34:36.765486Z","shell.execute_reply":"2024-05-30T11:34:36.769673Z"},"trusted":true},"execution_count":199,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import coo_matrix\n\n# Sparse matrix utils\ndef build_sparse(W_dict, shape):\n    W_row, W_col, W_data = [], [], []\n    for (current, nxt) in W_dict.keys():\n        W_row.append(current)\n        W_col.append(nxt)\n        W_data.append(W_dict[(current, nxt)])\n        \n    return sp.csr_matrix((W_data, (W_row, W_col)), shape= shape)\n\ndef ret_zero():\n    return 0\n\ndef csr2spMat(X_csr):\n    '''\n    Consult: https://stackoverflow.com/questions/50665141/converting-a-scipy-coo-matrix-to-pytorch-sparse-tensor\n    '''\n    X_coo = sp.csr_matrix.tocoo(X_csr, copy= True)\n    \n    values = X_coo.data\n    indices = np.vstack((X_coo.row, X_coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = X_coo.shape\n\n    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.772446Z","iopub.execute_input":"2024-05-30T11:34:36.772822Z","iopub.status.idle":"2024-05-30T11:34:36.786676Z","shell.execute_reply.started":"2024-05-30T11:34:36.772790Z","shell.execute_reply":"2024-05-30T11:34:36.785524Z"},"trusted":true},"execution_count":200,"outputs":[]},{"cell_type":"code","source":"# Statistics\ndef get_W(texts, window_size):\n    '''\n    Get the number of windows over texts.\n    <texts> is expected to have type of list(torch.Tensor)\n    \n    Batch_size: B = len(texts)\n    '''\n    res = 0\n    for text in texts:\n        res += max(len(text) - window_size + 1, 1)\n    return res\n\n\ndef build_W(texts, vocabulary, window_size, check= False):\n    '''\n    Build the co-occurence matrix W which will be represented using csr_matrix((data, (row_ind, col_ind))\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Batch size: B = len(texts)\n    Vocab size: V = len(vocabulary)\n    Number of Node: N = B + V\n    \n    Output:\n    -----\n    W_dict (defaultdict) : Edge-list for construction of W\n    W (sp.csr_matrix): Co-occurence matrix. Shape = (V, V)\n    mW (np.ndarray): Occurence vector. Can be viewed as marginallized W. Shape = (V)\n    '''\n    B = len(texts)\n    V = len(vocabulary)\n    N = B + V\n    mW = np.zeros((V)) # Margin W - Occurence vector\n    \n    W_dict = defaultdict(ret_zero) \n    \n    if check:\n        print('*********************************')\n        print('Tokenized texts:')\n    for text in texts:\n        L = len(text)\n        if check:\n            print(text)\n        for i in range(max(L - window_size + 1, 1)):\n            master_range = text[i : i + window_size]\n            mW[np.unique(master_range)] += 1\n            for j in range(window_size):\n                if i + j >= L:\n                    break\n                current = text[i+j]\n                W_dict[(current, current)] = W_dict[(current, current)] + 1\n                for nxt in master_range:\n                    if current != nxt:\n                        W_dict[(current, nxt)] = W_dict[(current, nxt)] + 1\n                    \n    W = build_sparse(W_dict, shape= (V, V))\n\n    return W_dict, W, mW\n\ndef calc_pmi(texts, vocabulary, window_size):\n    '''\n    Calculate PMI\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    \n    Output:\n    -----\n    pmi_dict (defaultdict) : Edge-list for construction of pmi\n    pmi (sp.csr_matrix): Pairwise PMI matrix. Shape = (V, V)\n    '''\n    # Preparations\n    nW = get_W(texts, window_size) # Number of windows\n    W_dict, W, mW = build_W(texts, vocabulary, window_size) # W_ij. Shape: (V, V)\n    \n    V = len(vocabulary)\n    pmi_dict = defaultdict(ret_zero)\n    margin_p = (mW / nW).reshape(-1, 1)\n    \n    # Constructing PMI edge list.\n    for (i, j) in W_dict.keys():\n        if i == j:\n            continue\n        pij = W_dict[(i, j)] / nW\n        pi = margin_p[i]\n        pj = margin_p[j]\n        \n        prelog = pij / (pi * pj)\n        if prelog <= 1:\n            continue\n        pmi_dict[(i, j)] = math.log2(prelog)\n        \n    pmi = build_sparse(pmi_dict, shape= (V, V))\n    \n    return pmi_dict, pmi\n    \n\ndef calc_tf_idf(texts, vocabulary):\n    '''\n    Calculate TF-IDFs\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    \n    NOTES:  For text graph constructions, words will be indexed from 0 to V-1 \n            while documents will be indexed from V to V + D - 1 \n    '''\n    V = len(vocabulary)\n    B = len(texts)\n    \n    tf_dict = defaultdict(ret_zero) # Term frequency - Shape: (B, V)\n    idf_dict = defaultdict(ret_zero) # Inverse document frequency - Shape: (V,)\n    tf_idf_dict = defaultdict(ret_zero) # TF - IDF - Shape: (B, V)\n    \n    idf = np.zeros((V)) # No. of documents containing token. The array is 0-based instead of V-based\n    \n    # Term frequency\n    for i, text in enumerate(texts):\n#         max_freq = 0\n        for j in range(text.shape[0]):\n            tf_dict[(V + i, text[j])] += 1\n#             max_freq = max(max_freq, tf_dict[(V + i, text[j])])\n            \n            idf_dict[(V + i, text[j])] = 1\n        # Normalize term frequency\n        for j in range(text.shape[0]):\n            tf_dict[(V + i, text[j])] /= len(text)\n    \n    # Inverse document frequency\n    for (i, j) in idf_dict.keys():\n        idf[j] += idf_dict[(i, j)]\n        \n    idf[0] = B # There is no unknown token. Hence, idf[<unk>] = 0\n    idf = np.log2(B / idf) # Shape: (V,)\n    \n    # TF - IDF\n    for (i, j) in tf_dict.keys():\n        tf_idf_dict[(i, j)] = tf_dict[(i, j)] * idf[j]\n\n    tf = build_sparse(tf_dict, shape= (B + V, V))\n    tf_idf = build_sparse(tf_idf_dict, shape= (B + V, V))\n    \n    return tf_dict, tf, idf, tf_idf_dict, tf_idf","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.791850Z","iopub.execute_input":"2024-05-30T11:34:36.792579Z","iopub.status.idle":"2024-05-30T11:34:36.819364Z","shell.execute_reply.started":"2024-05-30T11:34:36.792532Z","shell.execute_reply":"2024-05-30T11:34:36.818014Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"# Pipeline testing\nraw_texts = ['A two-layer GCN can allow message', \n             'A two-layer GCN can allow message', \n             'A two-layer GCN can allow message',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'In our preliminary experiment',\n             'In our preliminary experiment',\n             'In our preliminary experiment']\nvocabulary = get_vocab(raw_texts)\nprint('*********************************')\nprint('Token list:')\nprint(vocabulary.get_itos())\nprint('*********************************')\ntexts = []\nfor text in raw_texts:\n    texts.append(text_pipeline(text, vocabulary))\n    \nprint('Vocabulary size V = ', len(vocabulary))\n_, W, mW = build_W(texts, vocabulary, 5, True)\nprint('*********************************')\nprint('Shape of W:', W.shape, 'Number of stored values:', W.nnz)\nprint('Shape of mW:', mW.shape)\nprint('*********************************')\nprint('Co-occurence matrix W:')\nprint(W)\nprint('*********************************')\nprint('Margin-W:')\nprint(mW)\nprint('*********************************')\nprint('PMI:')\n_, pmi = calc_pmi(texts, vocabulary, 5)\nprint('Shape of PMI', pmi.shape, 'Number of stored values:', pmi.nnz)\nprint(pmi)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.821243Z","iopub.execute_input":"2024-05-30T11:34:36.822356Z","iopub.status.idle":"2024-05-30T11:34:36.940709Z","shell.execute_reply.started":"2024-05-30T11:34:36.822306Z","shell.execute_reply":"2024-05-30T11:34:36.939505Z"},"trusted":true},"execution_count":202,"outputs":[{"name":"stdout","text":"*********************************\nToken list:\n['<unk>', 'a', 'two-layer', 'gcn', 'can', 'allow', 'message', 'thus', 'although', 'there', 'is', 'no', 'direct', 'document-document', 'edges', 'in', 'the', 'graph', 'our', 'preliminary', 'experiment']\n*********************************\nVocabulary size V =  21\n*********************************\nTokenized texts:\n[1, 2, 3, 4, 5, 6]\n[1, 2, 3, 4, 5, 6]\n[1, 2, 3, 4, 5, 6]\n[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n[15, 18, 19, 20]\n[15, 18, 19, 20]\n[15, 18, 19, 20]\n*********************************\nShape of W: (21, 21) Number of stored values: 128\nShape of mW: (21,)\n*********************************\nCo-occurence matrix W:\n  (1, 1)\t3\n  (1, 2)\t3\n  (1, 3)\t3\n  (1, 4)\t3\n  (1, 5)\t3\n  (2, 1)\t3\n  (2, 2)\t6\n  (2, 3)\t6\n  (2, 4)\t6\n  (2, 5)\t6\n  (2, 6)\t3\n  (3, 1)\t3\n  (3, 2)\t6\n  (3, 3)\t6\n  (3, 4)\t6\n  (3, 5)\t6\n  (3, 6)\t3\n  (4, 1)\t3\n  (4, 2)\t6\n  (4, 3)\t6\n  (4, 4)\t6\n  (4, 5)\t6\n  (4, 6)\t3\n  (5, 1)\t3\n  (5, 2)\t6\n  :\t:\n  (15, 19)\t3\n  (15, 20)\t3\n  (16, 12)\t3\n  (16, 13)\t6\n  (16, 14)\t6\n  (16, 15)\t6\n  (16, 16)\t6\n  (16, 17)\t3\n  (17, 13)\t3\n  (17, 14)\t3\n  (17, 15)\t3\n  (17, 16)\t3\n  (17, 17)\t3\n  (18, 15)\t3\n  (18, 18)\t3\n  (18, 19)\t3\n  (18, 20)\t3\n  (19, 15)\t3\n  (19, 18)\t3\n  (19, 19)\t3\n  (19, 20)\t3\n  (20, 15)\t3\n  (20, 18)\t3\n  (20, 19)\t3\n  (20, 20)\t3\n*********************************\nMargin-W:\n[ 0.  3.  6.  6.  6.  6.  3.  3.  6.  9. 12. 15. 15. 15. 12. 12.  6.  3.\n  3.  3.  3.]\n*********************************\nPMI:\nShape of PMI (21, 21) Number of stored values: 92\n  (1, 2)\t2.321928094887362\n  (1, 3)\t2.321928094887362\n  (1, 4)\t2.321928094887362\n  (1, 5)\t2.321928094887362\n  (2, 1)\t2.321928094887362\n  (2, 3)\t2.321928094887362\n  (2, 4)\t2.321928094887362\n  (2, 5)\t2.321928094887362\n  (2, 6)\t2.321928094887362\n  (3, 1)\t2.321928094887362\n  (3, 2)\t2.321928094887362\n  (3, 4)\t2.321928094887362\n  (3, 5)\t2.321928094887362\n  (3, 6)\t2.321928094887362\n  (4, 1)\t2.321928094887362\n  (4, 2)\t2.321928094887362\n  (4, 3)\t2.321928094887362\n  (4, 5)\t2.321928094887362\n  (4, 6)\t2.321928094887362\n  (5, 1)\t2.321928094887362\n  (5, 2)\t2.321928094887362\n  (5, 3)\t2.321928094887362\n  (5, 4)\t2.321928094887362\n  (5, 6)\t2.321928094887362\n  (6, 2)\t2.321928094887362\n  :\t:\n  (14, 17)\t1.3219280948873622\n  (15, 13)\t0.584962500721156\n  (15, 14)\t0.9068905956085181\n  (15, 16)\t1.3219280948873622\n  (15, 17)\t1.3219280948873622\n  (15, 18)\t1.3219280948873622\n  (15, 19)\t1.3219280948873622\n  (15, 20)\t1.3219280948873622\n  (16, 13)\t1.0\n  (16, 14)\t1.3219280948873622\n  (16, 15)\t1.3219280948873622\n  (16, 17)\t2.321928094887362\n  (17, 13)\t1.0\n  (17, 14)\t1.3219280948873622\n  (17, 15)\t1.3219280948873622\n  (17, 16)\t2.321928094887362\n  (18, 15)\t1.3219280948873622\n  (18, 19)\t3.321928094887362\n  (18, 20)\t3.321928094887362\n  (19, 15)\t1.3219280948873622\n  (19, 18)\t3.321928094887362\n  (19, 20)\t3.321928094887362\n  (20, 15)\t1.3219280948873622\n  (20, 18)\t3.321928094887362\n  (20, 19)\t3.321928094887362\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3492411129.py:95: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  pmi_dict[(i, j)] = math.log2(prelog)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build Text graph\nThe adjacency matrix $A$ is designed as follow:\n\n\\begin{equation}\nA = \\begin{pmatrix}\nPMI & TF-IDF\\\\\nTF-IDF & 0\n\\end{pmatrix}\n\\end{equation}","metadata":{}},{"cell_type":"code","source":"def build_A(texts, vocabulary, window_size, nNodes):\n    '''\n    Calculate adjacency matrix A with shape (V+B, V+B)\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    Nodes: N = B + V\n    \n    '''\n    _, _, _, tf_idf_dict, _ = calc_tf_idf(texts, vocabulary)\n    pmi_dict, _ = calc_pmi(texts, vocabulary, window_size)\n    \n    B = len(texts)\n    V = len(vocabulary)\n    \n    N = B + V # Number of nodes\n    \n    A_dict = defaultdict(ret_zero) # Adjacency matrix. Shape: (N, N)\n    \n    # Word-word edges, obtained by pmi_dict\n    for x in pmi_dict.keys():\n        A_dict[x] = pmi_dict[x]\n        \n    # Document-word edges, obtained by tf_idf_dict\n    for x in tf_idf_dict.keys():\n        assert x[0] >= V and x[1] < V # Ensure edge (i, j) has i as document index, and j as token index\n        A_dict[x] = tf_idf_dict[x]\n        \n    # Main diagonal\n    for i in range(N):\n        A_dict[(i, i)] = 1\n    \n    if nNodes is None:\n        A = build_sparse(A_dict, shape= (N, N))\n    else:\n        A = build_sparse(A_dict, shape= (nNodes, nNodes))\n    \n    return A_dict, A\n\ndef build_graph(texts, vocabulary, window_size, nNodes= None):\n    '''\n    Build A_tilde = D x A x D\n    \n    Nodes: N\n    '''\n    B = len(texts)\n    V = len(vocabulary)\n    \n    N = B + V # Number of nodes\n    \n    A_dict, A = build_A(texts, vocabulary, window_size, nNodes= nNodes)\n    \n    D_dict = defaultdict(ret_zero)\n    \n    for (i, j) in A_dict.keys():\n        D_dict[(i, i)] = D_dict[(i, i)] + 1\n    for i in range(N):\n        D_dict[(i, i)] = (D_dict[(i, i)] ** (-0.5))\n    \n    if nNodes is None:\n        D = build_sparse(D_dict, shape= (N, N)) # Degree matrix\n    else:\n        D = build_sparse(D_dict, shape= (nNodes, nNodes))\n    G = (D * A) * D # A_tilde. Shape = (N, N)\n    \n    return G\n\ndef build_X(vocab, grand_vocab, batch_size, pretrained= True, W_e= None, init_dim= None):\n    '''\n    Construct graph feature input\n    <vocab>: Vocabulary constructed by batch\n    <grand_vocab>: Vocabulary created by pretrained embedding\n    \n    Embedding size: E\n    Batch size: B\n    Number of nodes: N\n    \n    Output:\n    -----\n    X (torch.Tensor, pretrained= True) : Embedded feature matrix. Shape = (N, N)\n    X (sp.csr_matrix, pretrained= False): Sparse one-hot feature matrix. Shape = (V, V)\n    '''\n    if pretrained:\n        assert W_e is not None\n        \n        X = []\n        \n        wordlist = vocab.get_itos()\n        E = W_e[0].shape[0]\n        B = batch_size\n        \n        # Convert relative vocab to grand_vocab\n        grand_vocab_id = [] # Shape: (V,)\n        for word in wordlist:\n            grand_vocab_id.append(grand_vocab[word])\n        \n        # Extract pretrained embeddings\n        Emb = W_e[grand_vocab_id] # Shape: (V, E)\n        X.append(Emb)\n        \n        # Document one-hot\n        Doc_Emb = torch.zeros(B, E)\n        Doc_Emb[0:B, 0:B] = 1\n        X.append(Doc_Emb.to(DEVICE))\n        \n        # Concat Word embeddings and Doc (one-hot) embeddings\n        X = torch.cat(X, dim= 0) # Shape: (V + B, E)\n        \n        return X\n    \n    else:\n        B = batch_size\n        \n        # Relative vocab is used as the main vocab in training phase. \n        # Given that no pretrained embedding is given.\n        wordlist = vocab.get_itos()\n        V = len(wordlist)\n        \n        # Number of nodes\n        N = B + V\n        \n        X_dict = defaultdict(ret_zero) # One-hot feature matrix X\n        for i in range(N):\n            X_dict[(i, i)] = 1\n        \n        X = build_sparse(X_dict, shape= (N, N))\n        \n        return X\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.942015Z","iopub.execute_input":"2024-05-30T11:34:36.942366Z","iopub.status.idle":"2024-05-30T11:34:36.968850Z","shell.execute_reply.started":"2024-05-30T11:34:36.942339Z","shell.execute_reply":"2024-05-30T11:34:36.967563Z"},"trusted":true},"execution_count":203,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Dataset class\nIn this section, the `YELPDataset` class is defined with custom batch function `collate_batch`.","metadata":{}},{"cell_type":"code","source":"# YELP Dataset\ndef collate_graph(batch):\n    '''\n    Collate batch and building text graph\n    '''\n    raw_texts = []\n    labels = []\n    mask = []\n    texts = []\n    for _text, _label in batch:\n        raw_texts.append(_text)\n        labels.append(_label)\n        \n    vocabulary = get_vocab(raw_texts)\n    \n    for _text in raw_texts:\n        texts.append(np.array(text_pipeline(_text, vocabulary)))\n        \n    return texts, labels, vocabulary\n\nclass YELPDataset(Dataset):\n    def __init__(self, df, df_sort= False):\n        self.df = df\n        if df_sort:\n            self.sort_df_by_txt_len()\n    \n    def sort_df_by_txt_len(self):\n        len_list = [-len(self.df.iloc[i]['text']) for i in range(len(self.df))]\n        self.df = self.df.iloc[np.argsort(len_list)]\n    \n    def __len__(self):\n        return len(self.df)\n\n    def label_pipeline(self, x):\n        return int(x)\n    \n    def __getitem__(self, idx):\n        txt = self.df.iloc[idx]['text']\n        \n        label = self.label_pipeline(self.df.iloc[idx]['label'])\n        \n        return (txt, label)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.970745Z","iopub.execute_input":"2024-05-30T11:34:36.971229Z","iopub.status.idle":"2024-05-30T11:34:36.987830Z","shell.execute_reply.started":"2024-05-30T11:34:36.971185Z","shell.execute_reply":"2024-05-30T11:34:36.986487Z"},"trusted":true},"execution_count":204,"outputs":[]},{"cell_type":"code","source":"# Dataset, Dataloader\ntrainset = YELPDataset(train_ds)\nvalset = YELPDataset(val_ds)\ntestset = YELPDataset(test_ds)\n\ntrainloader = DataLoader(trainset, batch_size= len(trainset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\nvalloader = DataLoader(valset, batch_size= len(valset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\ntestloader = DataLoader(testset, batch_size= len(testset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\nfor batch in trainloader:\n    X, y, vocabulary = batch\n    print(\"Shape of Texts:\", len(X), len(X[0]))\n    print(\"Shape of Labels:\", len(y))\n    print(\"Length of Vocab:\", len(vocabulary))\n    GLOBAL_N = len(vocabulary) + len(y) # ONLY ONE BIG GRAPH IS NEEDED FOR TRAINING !\n    print(\"Number of Nodes:\", GLOBAL_N)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:36.989334Z","iopub.execute_input":"2024-05-30T11:34:36.989731Z","iopub.status.idle":"2024-05-30T11:34:38.750139Z","shell.execute_reply.started":"2024-05-30T11:34:36.989699Z","shell.execute_reply":"2024-05-30T11:34:38.748999Z"},"trusted":true},"execution_count":205,"outputs":[{"name":"stdout","text":"Shape of Texts: 5879 39\nShape of Labels: 5879\nLength of Vocab: 15277\nNumber of Nodes: 21156\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TextGCN Module","metadata":{}},{"cell_type":"code","source":"# Taks configs\nWINDOW_SIZE = 20\nEMBEDDING_SIZE = 200\nNUM_CLASSES = 52\nDROPOUT = 0.5","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:38.752343Z","iopub.execute_input":"2024-05-30T11:34:38.752732Z","iopub.status.idle":"2024-05-30T11:34:38.759909Z","shell.execute_reply.started":"2024-05-30T11:34:38.752699Z","shell.execute_reply":"2024-05-30T11:34:38.757520Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"# TextGCN Module\nclass TextGCN(nn.Module):\n    def __init__(self, graph_emb_size, num_classes, dropout= 0.5, X_emb_size= None, sparse= True):\n        super(TextGCN, self).__init__()\n        self.sparse = sparse\n        self.E = graph_emb_size\n        \n        if not sparse:\n            self.layer = nn.Sequential(\n                nn.LazyLinear(self.E), # Lazy linear for flexible initial embedding\n                nn.Dropout(p= dropout),\n                nn.ReLU(),\n            )\n        else:\n            assert X_emb_size is not None\n            self.N = X_emb_size\n            self.W0 = nn.Parameter(torch.randn(int(X_emb_size), graph_emb_size))\n            self.afterlinear = nn.Sequential(\n                nn.Dropout(p= dropout),\n                nn.ReLU(),\n            )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(self.E, num_classes),\n            nn.Softmax(dim= 1)\n        )\n    \n    def forward(self, batch_size, G, X, check_shape= False):\n        '''\n        <G> : Graph matrix, expected to have shape (N, N) and symetric\n        <X> : Initial embedding. Expected to have shape (N, E_0)\n        \n        NOTES: If 'sparse' is True, G, X are given in scipy.sparse.csr_matrix format\n        \n        Vocab size: V\n        Batch size: B\n        Nodes: N = B + V\n        Graph embedding size: E\n        Initial embedding size: E_0\n        '''\n        N = G.shape[0]\n        E_0 = X.shape[1]\n        B = batch_size\n        V = N - B\n        \n        # First layer:\n        if not self.sparse:\n            L1 = self.layer(torch.matmul(G, X)) # Shape: (N, E_0)\n        else:\n            L1 = G * X # Shape: (N, N), type = sp.csr_matrix\n            L1 = csr2spMat(L1).to(DEVICE)\n            L1 = torch.sparse.mm(L1, self.W0) # Shape: (N, E), type = dense matrix\n            L1 = self.afterlinear(L1)\n        \n        # Classifier:\n        if not self.sparse:\n            logits = self.classifier(torch.matmul(G, L1)) # Shape: (N, num_classes)\n        else:\n            newG = csr2spMat(G).to(DEVICE)\n            logits = self.classifier(torch.sparse.mm(newG, L1).to(DEVICE)).to(DEVICE) # Shape: (N, num_classes)\n        \n        doc_logits = logits[V:, :] # Shape: (B, num_classes)\n        \n        if check_shape:\n            print('*********************************')\n            print('Shape of L1:', L1.shape)\n            print('Shape of logits:', logits.shape)\n            print('Shape of doc_logits:', doc_logits.shape)\n        \n        return logits\n    \ndef init_weights(m):\n    '''\n    Kaiming initialization.\n    '''\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Parameter):\n        torch.nn.init.kaiming_normal(m.weight, nonlinearity='relu')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:38.762533Z","iopub.execute_input":"2024-05-30T11:34:38.763000Z","iopub.status.idle":"2024-05-30T11:34:38.788547Z","shell.execute_reply.started":"2024-05-30T11:34:38.762956Z","shell.execute_reply":"2024-05-30T11:34:38.787416Z"},"trusted":true},"execution_count":207,"outputs":[]},{"cell_type":"code","source":"# Model initialization\nmodel = TextGCN(EMBEDDING_SIZE, NUM_CLASSES, DROPOUT, GLOBAL_N).to(DEVICE)\nmodel.apply(init_weights)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:38.790238Z","iopub.execute_input":"2024-05-30T11:34:38.791368Z","iopub.status.idle":"2024-05-30T11:34:38.859050Z","shell.execute_reply.started":"2024-05-30T11:34:38.791321Z","shell.execute_reply":"2024-05-30T11:34:38.857842Z"},"trusted":true},"execution_count":208,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/4233593718.py:77: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n  torch.nn.init.kaiming_normal(m.weight, nonlinearity='relu')\n","output_type":"stream"},{"execution_count":208,"output_type":"execute_result","data":{"text/plain":"TextGCN(\n  (afterlinear): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): ReLU()\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=200, out_features=52, bias=True)\n    (1): Softmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training\n\n## Prepare text graphs\nGenerating text graphs from text corpus is a considerable computational bottleneck. Hence, the graph $G$ and input $X$ should be calculated in advance. Following the original paper, $X$ is constructed using one-hot encoding over vocabulary and documents inferred from the corpus. Hence, $X\\in\\mathbb{R}^{N\\times N}$ where $N = B + V$, with $V$ being vocabulary size and $B$ being number of documents.\n\nHence, during inference time, texts must be load in a batch of size $B$ and tokenize using the training vocabulary in order to maintain dimensional consistency with pretrained **TextGCN** model.","metadata":{}},{"cell_type":"code","source":"# Preparing text graphs\n\n# Train graphs\nG_trainlist = []\nX_trainlist = []\ny_trainlist = []\n\nfor i, (texts, labels, vocabulary) in enumerate(trainloader):\n    # Build graph\n    G = build_graph(texts, vocabulary, WINDOW_SIZE) # Graph matrix\n    N = G.shape[0] # Number of nodes\n    \n    print('Number of nodes: N = ', N)\n    torch.save(vocabulary, 'train_vocab.pth')\n\n    # Input features.\n    # X = build_X(vocabulary, grand_vocab, len(labels), W_e= W_e) # Pretrained embedding mode. Not efficient.\n    # One-hot encoding mode\n    X_dict = defaultdict(ret_zero)\n    for i in range(N):\n        X_dict[(i, i)] = 1\n    X = build_sparse(X_dict, shape= (N, N))\n    y = torch.tensor(labels)\n    \n    G_trainlist.append(G)\n    X_trainlist.append(X)\n    y_trainlist.append(y)\n    \n    break # Only ONE BIG GRAPH is needed for training\n\nGXy_train = (G_trainlist, X_trainlist, y_trainlist)\ntorch.save(GXy_train, \"GXy_train.pth\")\n\n# Val graphs\nG_vallist = []\nX_vallist = []\ny_vallist = []\n\nfor i, (texts, labels, vocabulary) in enumerate(valloader):\n    # Build graph\n    G = build_graph(texts, vocabulary, WINDOW_SIZE, nNodes= GLOBAL_N) # Graph matrix\n    N = G.shape[0] # Number of nodes\n    \n    print('Number of nodes: N = ', N)\n\n    # Input features.\n    # X = build_X(vocabulary, grand_vocab, len(labels), W_e= W_e) # Pretrained embedding mode. Not efficient.\n    # One-hot encoding mode\n    X_dict = defaultdict(ret_zero)\n    for i in range(N):\n        X_dict[(i, i)] = 1\n    X = build_sparse(X_dict, shape= (GLOBAL_N, GLOBAL_N))\n    y = torch.tensor(labels)\n    \n    G_vallist.append(G)\n    X_vallist.append(X)\n    y_vallist.append(y)\n    \n    break # Only ONE BIG GRAPH is needed for training\n\nGXy_val = (G_vallist, X_vallist, y_vallist)\ntorch.save(GXy_val, \"GXy_val.pth\")\n\n# Test graphs\nG_testlist = []\nX_testlist = []\ny_testlist = []\n\nfor i, (texts, labels, vocabulary) in enumerate(testloader):\n    # Build graph\n    G = build_graph(texts, vocabulary, WINDOW_SIZE, nNodes= GLOBAL_N) # Graph matrix\n    N = G.shape[0] # Number of nodes\n    \n    print('Number of nodes: N = ', N)\n\n    # Input features.\n    # X = build_X(vocabulary, grand_vocab, len(labels), W_e= W_e) # Pretrained embedding mode. Not efficient.\n    # One-hot encoding mode\n    X_dict = defaultdict(ret_zero)\n    for i in range(N):\n        X_dict[(i, i)] = 1\n    X = build_sparse(X_dict, shape= (GLOBAL_N, GLOBAL_N))\n    y = torch.tensor(labels)\n    \n    G_testlist.append(G)\n    X_testlist.append(X)\n    y_testlist.append(y)\n    \n    break # Only ONE BIG GRAPH is needed for training\n\nGXy_test = (G_testlist, X_testlist, y_testlist)\ntorch.save(GXy_test, \"GXy_test.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:34:38.860654Z","iopub.execute_input":"2024-05-30T11:34:38.860967Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/3492411129.py:95: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  pmi_dict[(i, j)] = math.log2(prelog)\n","output_type":"stream"},{"name":"stdout","text":"Number of nodes: N =  21156\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Load prebuilt graph if available\n# GXy_train = torch.load('/kaggle/input/gxy-r52/GXy_train.pth')\n# GXy_val = torch.load('/kaggle/input/gxy-r52/GXy_val.pth')\n# GXy_test = torch.load('/kaggle/input/gxy-r52/GXy_test.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training configs","metadata":{}},{"cell_type":"code","source":"# Training configs\nLR = 0.02\n\nEPOCHS = 200\nITER = EPOCHS\n\nOPTIMIZER = torch.optim.Adam(model.parameters(), lr= LR)\n# OPTIMIZER = torch.optim.SGD(model.parameters(), lr= LR, momentum= 0.9, nesterov= True)\n\n# SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = ITER)\nSCHEDULER = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(OPTIMIZER, T_0= 25, T_mult = 1, eta_min= 0)\n\nLOSS_FN = nn.CrossEntropyLoss()\nRECORD = 0.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training procedures","metadata":{}},{"cell_type":"code","source":"# Train procedures\ndef test(GXy, B_train, model, loss_fn):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    G_list, X_list, y_list = GXy\n    L = len(G_list)\n    n_samples = 0\n    \n    for i in tqdm(range(L)):\n        G, X, y = G_list[i], X_list[i], y_list[i]\n        y = y.to(DEVICE)\n        batch_size = y.shape[0]\n        N = G.shape[0]\n        \n        # Get loss and inference\n        logits = model(batch_size, G, X)\n#         print(logits.shape, N, N - B_train, N - B_train + batch_size)\n        logits = logits[N - B_train : N - B_train + batch_size, :]\n#         print(logits.shape, y.shape)\n        loss = loss_fn(logits, y)\n        n_samples += (batch_size)\n        correct += (logits.argmax(dim= 1) == y).type(torch.float).sum().item()\n\n        test_loss += loss\n        \n    test_loss /= L\n    accuracy = correct / n_samples\n    \n    return test_loss, accuracy\n\ndef train(GXy_train, B_train, model, optimizer, scheduler, loss_fn, val_freq):\n    global RECORD\n    model.train()\n    tloss = []\n    \n    G_list, X_list, y_list = GXy_train\n    L = len(G_list)\n    \n    for i in range(L):\n        G, X, y = G_list[i], X_list[i], y_list[i]\n        y = y.to(DEVICE)\n        batch_size = y.shape[0]\n        N = G.shape[0]\n        \n        # Get loss\n        logits = model(batch_size, G, X)\n        logits = logits[N - B_train : N - B_train + batch_size, :]\n        loss = loss_fn(logits, y)\n        \n        tloss.append(loss.cpu().detach().numpy())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n#         scheduler.step()\n        \n    return tloss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING \niter_loss = []\nepoch_loss = []\nbest_acc = 0\n\nwith torch.no_grad():\n    pass\n\nfor t in range(EPOCHS):\n    tloss_train = train(GXy_train, len(train_ds), model, OPTIMIZER, SCHEDULER, LOSS_FN, 10)\n    \n    iter_loss = iter_loss + tloss_train\n#     epoch_loss.append(sum(tloss) / len(tloss))\n    \n#     print(f'Epoch {t}: LOSS = {epoch_loss[-1]}')\n    \n    tloss_val = train(GXy_val, len(train_ds), model, OPTIMIZER, SCHEDULER, LOSS_FN, 10)\n    \n    iter_loss = iter_loss + tloss_val\n    epoch_loss.append(sum(tloss_train + tloss_val) / len(tloss_train + tloss_val))\n    \n    print(f'Epoch {t}: LOSS = {epoch_loss[-1]}')\n    \ntorch.save(model.state_dict(), f'TextGCN_last.pth')\n    \nfig, axes = plt.subplots()\naxes.plot(epoch_loss, label = 'train-loss')\naxes.legend()\naxes.set_xlabel('Iteration')\naxes.set_ylabel('Loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\nloss = LOSS_FN(inputs, target)\nprint(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verify training accuracy","metadata":{}},{"cell_type":"code","source":"# GXy_train[0][0].to(CPU), GXy_train[1][0].to(CPU), GXy_train[2][0].to(CPU)\n_, test_acc = test(GXy_val, len(train_ds), model, LOSS_FN)\nprint(f'Train accuracy: {test_acc}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}