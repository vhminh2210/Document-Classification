{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"final code.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8528311,"sourceType":"datasetVersion","datasetId":5093015}],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TextGCN\nConsult: https://github.com/usydnlp/TextGCN_analysis/tree/main for more detail","metadata":{}},{"cell_type":"markdown","source":"## Preamble","metadata":{}},{"cell_type":"code","source":"# Preamble\nimport time, random\nimport re, string\nimport os, sys\nimport math\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:20:51.248662Z","iopub.execute_input":"2024-06-01T14:20:51.249079Z","iopub.status.idle":"2024-06-01T14:20:51.255388Z","shell.execute_reply.started":"2024-06-01T14:20:51.249050Z","shell.execute_reply":"2024-06-01T14:20:51.254410Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preparation","metadata":{"id":"hJPI-IXrBkrP"}},{"cell_type":"code","source":"# # YELP Preparation\n# data_file = open(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\")\n# data = []\n\n# # cnt = 1569264 # Size of YELP 2015 dataset\n# cnt = 100000\n\n# for line in data_file:\n#     data.append(json.loads(line))\n#     cnt -= 1\n#     if cnt == 0:\n#         break\n    \n# data_file.close()\n# df = pd.DataFrame(data)\n\n# print(\"Number of datapoints:\", len(df))\n# df.head()\n\n# # R52 Preparation\n# train_ds = pd.read_csv('/kaggle/input/smolcsv/r52-train-stemmed.csv')\n# val_ds = pd.read_csv('/kaggle/input/smolcsv/r52-dev-stemmed.csv')\n# test_ds = pd.read_csv('/kaggle/input/smolcsv/r52-test-stemmed.csv')\n\n# ds = pd.concat([train_ds, val_ds, test_ds])\n\n# print('Size of data corpus:', len(ds))\n# ds.head()\n\n# MR Preparation\nodf = pd.read_csv('/kaggle/input/smolcsv/MR.csv')\n\nprint(\"Number of datapoints:\", len(odf))\nodf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:20:51.257504Z","iopub.execute_input":"2024-06-01T14:20:51.257850Z","iopub.status.idle":"2024-06-01T14:20:51.305086Z","shell.execute_reply.started":"2024-06-01T14:20:51.257820Z","shell.execute_reply":"2024-06-01T14:20:51.304146Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Number of datapoints: 10662\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text Sentiment  \\\n0           0  the rock is destined to be the 21st century's ...  positive   \n1           1  the gorgeously elaborate continuation of \" the...  positive   \n2           2                     effective but too-tepid biopic  positive   \n3           3  if you sometimes like to go to the movies to h...  positive   \n4           4  emerges as something rare , an issue movie tha...  positive   \n\n   label  \n0      1  \n1      1  \n2      1  \n3      1  \n4      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>Sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>the rock is destined to be the 21st century's ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>the gorgeously elaborate continuation of \" the...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>effective but too-tepid biopic</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>if you sometimes like to go to the movies to h...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>emerges as something rare , an issue movie tha...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# MR preparation\ndf_size = len(odf)\nidx = [x for x in range(df_size)]\nrandom.Random(555).shuffle(idx)\n\ntrain_num = int(df_size * 0.8)\nval_num = int(df_size * 0.1)\ntest_num = int(df_size * 0.1)\n\n# print(train_num, val_num, test_num)\n\ntrain_idx = idx[:train_num]\nval_idx = idx[train_num : (train_num + val_num)]\ntest_idx = idx[(train_num + val_num) : ]\n\ntrain_df = odf.iloc[train_idx].reset_index()\nval_df = odf.iloc[val_idx].reset_index()\ntest_df = odf.iloc[test_idx].reset_index()\n\n# train_df = pd.read_csv('/kaggle/input/smolcsv/r52-train-stemmed.csv')\n# val_df = pd.read_csv('/kaggle/input/smolcsv/r52-dev-stemmed.csv')\n# test_df = pd.read_csv('/kaggle/input/smolcsv/r52-test-stemmed.csv')\n\nprint('Size of trainset:', len(train_df))\nprint('Size of valset:', len(val_df))\nprint('Size of testset:', len(test_df))\n\ndf = pd.concat([train_df, val_df, test_df])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:20:51.306207Z","iopub.execute_input":"2024-06-01T14:20:51.306453Z","iopub.status.idle":"2024-06-01T14:20:51.330401Z","shell.execute_reply.started":"2024-06-01T14:20:51.306432Z","shell.execute_reply":"2024-06-01T14:20:51.329530Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Size of trainset: 8529\nSize of valset: 1066\nSize of testset: 1067\n","output_type":"stream"}]},{"cell_type":"code","source":"# Clean data\ntrain_corpus = []\ntrain_label = []\ntest_corpus = []\ntest_label = []\nfor i in range(len(train_df)):\n    train_corpus.append(train_df.iloc[i]['text'])\n    train_label.append(train_df.iloc[i]['label'])\nfor i in range(len(val_df)):\n    train_corpus.append(val_df.iloc[i]['text'])\n    train_label.append(val_df.iloc[i]['label'])\nfor i in range(len(test_df)):\n    test_corpus.append(test_df.iloc[i]['text'])\n    test_label.append(test_df.iloc[i]['label'])","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:20:51.332371Z","iopub.execute_input":"2024-06-01T14:20:51.332644Z","iopub.status.idle":"2024-06-01T14:20:52.521277Z","shell.execute_reply.started":"2024-06-01T14:20:51.332622Z","shell.execute_reply":"2024-06-01T14:20:52.520449Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"original_train_sentences = train_corpus\noriginal_labels_train = train_label\noriginal_test_sentences = test_corpus\noriginal_labels_test = test_label\n\n# example \n# original_train_sentences = ['this is sample 1','this is sample 2']\n# original_labels_train = ['postive','negative']\n# original_test_sentences = ['this is sample 1','this is sample 2']\n# original_labels_test = ['postive','negative']\n\ntrain_size = len(original_train_sentences)\ntest_size = len(original_test_sentences)\nsentences = original_train_sentences + original_test_sentences","metadata":{"id":"-GyzNkI7W03D","execution":{"iopub.status.busy":"2024-06-01T14:20:52.524904Z","iopub.execute_input":"2024-06-01T14:20:52.525528Z","iopub.status.idle":"2024-06-01T14:20:52.531310Z","shell.execute_reply.started":"2024-06-01T14:20:52.525500Z","shell.execute_reply":"2024-06-01T14:20:52.530325Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Hyper Parameters","metadata":{"id":"6K9dWTv5I07_"}},{"cell_type":"code","source":"EDGE = 2 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\nNODE = 0 # 0:one-hot #1:BERT \nNUM_LAYERS = 2 \n\nHIDDEN_DIM = 200\nDROP_OUT = 0.5\nLR = 0.02\nWEIGHT_DECAY = 0\nEARLY_STOPPING = 10\nNUM_EPOCHS = 200","metadata":{"execution":{"iopub.status.busy":"2024-06-01T14:20:52.532606Z","iopub.execute_input":"2024-06-01T14:20:52.532961Z","iopub.status.idle":"2024-06-01T14:20:52.543650Z","shell.execute_reply.started":"2024-06-01T14:20:52.532920Z","shell.execute_reply":"2024-06-01T14:20:52.542865Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{"id":"a2W7wKTBfa71"}},{"cell_type":"markdown","source":"## Label Encoding","metadata":{"id":"hobYcJ5OX5oT"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\nunique_labels=np.unique(original_labels_train)\n\nnum_class = len(unique_labels)\nlEnc = LabelEncoder()\nlEnc.fit(unique_labels)\n\nprint(unique_labels)\nprint(lEnc.transform(unique_labels))\n\ntrain_labels = lEnc.transform(original_labels_train)\ntest_labels = lEnc.transform(original_labels_test)\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlabels = train_labels.tolist()+test_labels.tolist()\nlabels = torch.LongTensor(labels).to(device)","metadata":{"id":"PtWyhXiueMOq","execution":{"iopub.status.busy":"2024-06-01T14:20:52.544895Z","iopub.execute_input":"2024-06-01T14:20:52.545596Z","iopub.status.idle":"2024-06-01T14:20:52.573243Z","shell.execute_reply.started":"2024-06-01T14:20:52.545564Z","shell.execute_reply":"2024-06-01T14:20:52.572393Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[0 1]\n[0 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Remove Stopwords and less frequent words, tokenize sentences","metadata":{"id":"ZMkEBxr6fMQi"}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\nimport nltk\nimport re\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nremove_limit = 5\n\n\ndef clean_str(string):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\n\noriginal_word_freq = {}  # to remove rare words\nfor sentence in sentences:\n    temp = clean_str(sentence)\n    word_list = temp.split()\n    for word in word_list:\n        if word in original_word_freq:\n            original_word_freq[word] += 1\n        else:\n            original_word_freq[word] = 1   \n\ntokenize_sentences = []\nword_list_dict = {}\nfor sentence in sentences:\n    temp = clean_str(sentence)\n    word_list_temp = temp.split()\n    doc_words = []\n    for word in word_list_temp: \n        if word in original_word_freq:\n            doc_words.append(word)\n            word_list_dict[word] = 1\n    tokenize_sentences.append(doc_words)\nword_list = list(word_list_dict.keys())\nvocab_length = len(word_list)\n\n#word to id dict\nword_id_map = {}\nfor i in range(vocab_length):\n    word_id_map[word_list[i]] = i            ","metadata":{"id":"1xRG94uDfaBV","execution":{"iopub.status.busy":"2024-06-01T14:20:52.574341Z","iopub.execute_input":"2024-06-01T14:20:52.575159Z","iopub.status.idle":"2024-06-01T14:20:53.617165Z","shell.execute_reply.started":"2024-06-01T14:20:52.575126Z","shell.execute_reply":"2024-06-01T14:20:53.616337Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"node_size = train_size + vocab_length + test_size","metadata":{"id":"dqLUncB2Pn_L","execution":{"iopub.status.busy":"2024-06-01T14:20:53.618270Z","iopub.execute_input":"2024-06-01T14:20:53.618615Z","iopub.status.idle":"2024-06-01T14:20:53.622976Z","shell.execute_reply.started":"2024-06-01T14:20:53.618585Z","shell.execute_reply":"2024-06-01T14:20:53.622017Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Model input","metadata":{"id":"g0o8wcXgrTiD"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm","metadata":{"id":"EZbRV2wYxY1U","execution":{"iopub.status.busy":"2024-06-01T14:20:53.624065Z","iopub.execute_input":"2024-06-01T14:20:53.624327Z","iopub.status.idle":"2024-06-01T14:20:53.633477Z","shell.execute_reply.started":"2024-06-01T14:20:53.624305Z","shell.execute_reply":"2024-06-01T14:20:53.632670Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Build Graph","metadata":{"id":"znJ7Grz7fQ2L"}},{"cell_type":"code","source":"from math import log\nrow = []\ncol = []\nweight = []","metadata":{"id":"-BSg1uNgV3_7","execution":{"iopub.status.busy":"2024-06-01T14:20:53.634481Z","iopub.execute_input":"2024-06-01T14:20:53.634736Z","iopub.status.idle":"2024-06-01T14:20:53.681318Z","shell.execute_reply.started":"2024-06-01T14:20:53.634713Z","shell.execute_reply":"2024-06-01T14:20:53.680138Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### word-word: PMI","metadata":{"id":"QESQPT88AqsI"}},{"cell_type":"code","source":"if EDGE >= 1:\n    window_size = 20\n    total_W = 0\n    word_occurrence = {}\n    word_pair_occurrence = {}\n\n    def ordered_word_pair(a, b):\n        if a > b:\n            return b, a\n        else:\n            return a, b\n\n    def update_word_and_word_pair_occurrence(q):\n        unique_q = list(set(q))\n        for i in unique_q:\n            try:\n                word_occurrence[i] += 1\n            except:\n                word_occurrence[i] = 1\n        for i in range(len(unique_q)):\n            for j in range(i+1, len(unique_q)):\n                word1 = unique_q[i]\n                word2 = unique_q[j]\n                word1, word2 = ordered_word_pair(word1, word2)\n                try:\n                    word_pair_occurrence[(word1, word2)] += 1\n                except:\n                    word_pair_occurrence[(word1, word2)] = 1\n\n\n    for ind in tqdm(range(train_size+test_size)):\n        words = tokenize_sentences[ind]\n\n        q = []\n        # push the first (window_size) words into a queue\n        for i in range(min(window_size, len(words))):\n            q += [word_id_map[words[i]]]\n        # update the total number of the sliding windows\n        total_W += 1\n        # update the number of sliding windows that contain each word and word pair\n        update_word_and_word_pair_occurrence(q)\n\n        now_next_word_index = window_size\n        # pop the first word out and let the next word in, keep doing this until the end of the document\n        while now_next_word_index<len(words):\n            q.pop(0)\n            q += [word_id_map[words[now_next_word_index]]]\n            now_next_word_index += 1\n            # update the total number of the sliding windows\n            total_W += 1\n            # update the number of sliding windows that contain each word and word pair\n            update_word_and_word_pair_occurrence(q)\n\n    for word_pair in word_pair_occurrence:\n        i = word_pair[0]\n        j = word_pair[1]\n        count = word_pair_occurrence[word_pair]\n        word_freq_i = word_occurrence[i]\n        word_freq_j = word_occurrence[j]\n        pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n        if pmi <=0:\n            continue\n        row.append(train_size + i)\n        col.append(train_size + j)\n        weight.append(pmi)\n        row.append(train_size + j)\n        col.append(train_size + i)\n        weight.append(pmi)\n","metadata":{"id":"KNlJoLFagXhv","execution":{"iopub.status.busy":"2024-06-01T14:20:53.682869Z","iopub.execute_input":"2024-06-01T14:20:53.683848Z","iopub.status.idle":"2024-06-01T14:21:02.579988Z","shell.execute_reply.started":"2024-06-01T14:20:53.683812Z","shell.execute_reply":"2024-06-01T14:21:02.578987Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10662 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0d47eed5c14f3cb1a28599782f3f24"}},"metadata":{}}]},{"cell_type":"markdown","source":"### doc-word: Tf-idf","metadata":{"id":"hynLnT3a33kW"}},{"cell_type":"code","source":"#get each word appears in which document\nword_doc_list = {}\nfor word in word_list:\n    word_doc_list[word]=[]\n\nfor i in range(len(tokenize_sentences)):\n    doc_words = tokenize_sentences[i]\n    unique_words = set(doc_words)\n    for word in unique_words:\n        exsit_list = word_doc_list[word]\n        exsit_list.append(i)\n        word_doc_list[word] = exsit_list\n\n#document frequency\nword_doc_freq = {}\nfor word, doc_list in word_doc_list.items():\n    word_doc_freq[word] = len(doc_list)\n\n# term frequency\ndoc_word_freq = {}\n\nfor doc_id in range(len(tokenize_sentences)):\n    words = tokenize_sentences[doc_id]\n    for word in words:\n        word_id = word_id_map[word]\n        doc_word_str = str(doc_id) + ',' + str(word_id)\n        if doc_word_str in doc_word_freq:\n            doc_word_freq[doc_word_str] += 1\n        else:\n            doc_word_freq[doc_word_str] = 1","metadata":{"id":"BnSPqhg1lHps","execution":{"iopub.status.busy":"2024-06-01T14:21:02.581428Z","iopub.execute_input":"2024-06-01T14:21:02.581801Z","iopub.status.idle":"2024-06-01T14:21:03.010269Z","shell.execute_reply.started":"2024-06-01T14:21:02.581747Z","shell.execute_reply":"2024-06-01T14:21:03.009505Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for i in range(len(tokenize_sentences)):\n    words = tokenize_sentences[i]\n    doc_word_set = set()\n    for word in words:\n        if word in doc_word_set:\n            continue\n        j = word_id_map[word]\n        key = str(i) + ',' + str(j)\n        freq = doc_word_freq[key]\n        if i < train_size:\n            row.append(i)\n        else:\n            row.append(i + vocab_length)\n        col.append(train_size + j)\n        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n        weight.append(freq * idf)\n        doc_word_set.add(word)","metadata":{"id":"Z6elPPFO_sXp","execution":{"iopub.status.busy":"2024-06-01T14:21:03.014677Z","iopub.execute_input":"2024-06-01T14:21:03.015026Z","iopub.status.idle":"2024-06-01T14:21:03.629487Z","shell.execute_reply.started":"2024-06-01T14:21:03.015003Z","shell.execute_reply":"2024-06-01T14:21:03.628708Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"### doc-doc: jaccard","metadata":{"id":"FAr6ygKhWTc-"}},{"cell_type":"code","source":"import nltk\n\nif EDGE>=2:\n    tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n    jaccard_threshold = 0.2\n    for i in tqdm(range(len(tokenize_sentences))):\n        for j in range(i+1, len(tokenize_sentences)):\n            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n            if jaccard_w > jaccard_threshold:\n                if i < train_size:\n                    row.append(i)\n                else:\n                    row.append(i + vocab_length)\n                if j < train_size:\n                    col.append(j)\n                else:\n                    col.append(vocab_length + j)\n                weight.append(jaccard_w)\n                if j < train_size:\n                    row.append(j)\n                else:\n                    row.append(j + vocab_length)\n                if i < train_size:\n                    col.append(i)\n                else:\n                    col.append(vocab_length + i)\n                weight.append(jaccard_w)","metadata":{"id":"T4-EH15oWWSX","execution":{"iopub.status.busy":"2024-06-01T14:21:03.630756Z","iopub.execute_input":"2024-06-01T14:21:03.631353Z","iopub.status.idle":"2024-06-01T14:24:54.431449Z","shell.execute_reply.started":"2024-06-01T14:21:03.631320Z","shell.execute_reply":"2024-06-01T14:24:54.430594Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10662 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05f3785c44664c8eb9ae46e41016efc0"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Adjacent matrix","metadata":{"id":"uIkGgB2aZDk7"}},{"cell_type":"code","source":"import scipy.sparse as sp\nadj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n\n# build symmetric adjacency matrix\nadj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)","metadata":{"id":"C0O1Ucdhod9a","execution":{"iopub.status.busy":"2024-06-01T14:24:54.432914Z","iopub.execute_input":"2024-06-01T14:24:54.433199Z","iopub.status.idle":"2024-06-01T14:24:55.645260Z","shell.execute_reply.started":"2024-06-01T14:24:54.433172Z","shell.execute_reply":"2024-06-01T14:24:55.644452Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def normalize_adj(adj):\n    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n    \nadj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n\nadj = sparse_mx_to_torch_sparse_tensor(adj)","metadata":{"id":"ivyuexATkQFW","execution":{"iopub.status.busy":"2024-06-01T14:24:55.646372Z","iopub.execute_input":"2024-06-01T14:24:55.646674Z","iopub.status.idle":"2024-06-01T14:24:56.040814Z","shell.execute_reply.started":"2024-06-01T14:24:55.646650Z","shell.execute_reply":"2024-06-01T14:24:56.039996Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3533750465.py:20: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:605.)\n  return torch.sparse.FloatTensor(indices, values, shape).to(device)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Features","metadata":{"id":"pMgbhTstMSUA"}},{"cell_type":"code","source":"if NODE == 0:\n    features = np.arange(node_size)\n    features = torch.FloatTensor(features).to(device)\nelse:\n    \n    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n    from flair.data import Sentence\n    doc_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n    word_embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1',subtoken_pooling=\"mean\")\n\n    sent_embs = []\n    word_embs = {}\n\n    for ind in tqdm(range(train_size+test_size)):\n        sent = tokenize_sentences[ind]\n        sentence = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n        doc_embedding.embed(sentence)\n        sent_embs.append(sentence.get_embedding().tolist())\n        words = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n        word_embedding.embed(words)\n        for token in words:\n            word = token.text\n            embedding = token.embedding.tolist()\n            if word not in word_embs:\n                word_embs[word] = embedding\n            else:\n                word_embs[word] = np.minimum(word_embs[word], embedding)\n\n    word_embs_list = []\n    for word in word_list:\n        word_embs_list.append(word_embs[word])\n\n    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n\n    import scipy.sparse as sp\n    def preprocess_features(features):\n        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n        rowsum = np.array(features.sum(1))\n        r_inv = np.power(rowsum, -1).flatten()\n        r_inv[np.isinf(r_inv)] = 0.\n        r_mat_inv = sp.diags(r_inv)\n        features = r_mat_inv.dot(features)\n        return features\n\n    features = preprocess_features(sp.csr_matrix(features)).todense()\n    features = torch.FloatTensor(features).to(device)","metadata":{"id":"mP9dqCskOrXT","execution":{"iopub.status.busy":"2024-06-01T14:24:56.042090Z","iopub.execute_input":"2024-06-01T14:24:56.042462Z","iopub.status.idle":"2024-06-01T14:24:56.058685Z","shell.execute_reply.started":"2024-06-01T14:24:56.042430Z","shell.execute_reply":"2024-06-01T14:24:56.057968Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"pdx6RrUvjbF0"}},{"cell_type":"markdown","source":"## GCN Layer","metadata":{"id":"39Kj8NQujiDH"}},{"cell_type":"code","source":"import math\n\nimport torch\n\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.module import Module\n\n\nclass GraphConvolution(Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = Parameter(torch.zeros(1, out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters(in_features, out_features)\n        self.dropout = torch.nn.Dropout(drop_out)\n        self.activation =  activation\n\n    def reset_parameters(self,in_features, out_features):\n        stdv = np.sqrt(6.0/(in_features+out_features))\n        # stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        # if self.bias is not None:\n        #     torch.nn.init.zeros_(self.bias)\n            # self.bias.data.uniform_(-stdv, stdv)\n\n\n    def forward(self, input, adj, feature_less = False):\n        if feature_less:\n            support = self.weight\n            support = self.dropout(support)\n        else:\n            input = self.dropout(input)\n            support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            output = output + self.bias\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'","metadata":{"id":"jNVkA-h7b3sP","execution":{"iopub.status.busy":"2024-06-01T14:24:56.059740Z","iopub.execute_input":"2024-06-01T14:24:56.060072Z","iopub.status.idle":"2024-06-01T14:24:56.072489Z","shell.execute_reply.started":"2024-06-01T14:24:56.060042Z","shell.execute_reply":"2024-06-01T14:24:56.071527Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## GCN Model","metadata":{"id":"k57M4sz4s4Md"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass GCN(nn.Module):\n    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n        super(GCN, self).__init__()\n        self.n_layers = n_layers\n        self.gc_list = []\n        if n_layers >= 2:\n            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n            self.gcf = GraphConvolution(nhid, nclass, dropout)\n        else:\n            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n\n    def forward(self, x, adj):\n        if self.n_layers>=2:\n            x = self.gc1(x, adj, feature_less = True)\n            for i in range(self.n_layers-2):\n                x = self.gc_list[i](x,adj)\n            x = self.gcf(x,adj)\n        else:\n            x = self.gc1(x, adj, feature_less = True)\n        return x","metadata":{"id":"aJ-ZQuMzs5tZ","execution":{"iopub.status.busy":"2024-06-01T14:24:56.073539Z","iopub.execute_input":"2024-06-01T14:24:56.073815Z","iopub.status.idle":"2024-06-01T14:24:56.086795Z","shell.execute_reply.started":"2024-06-01T14:24:56.073788Z","shell.execute_reply":"2024-06-01T14:24:56.085957Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def cal_accuracy(predictions,labels):\n    pred = torch.argmax(predictions,-1).cpu().tolist()\n    lab = labels.cpu().tolist()\n    cor = 0\n    for i in range(len(pred)):\n        if pred[i] == lab[i]:\n            cor += 1\n    return cor/len(pred)","metadata":{"id":"qmhOG1yG--Ji","execution":{"iopub.status.busy":"2024-06-01T14:24:56.087890Z","iopub.execute_input":"2024-06-01T14:24:56.088652Z","iopub.status.idle":"2024-06-01T14:24:56.100394Z","shell.execute_reply.started":"2024-06-01T14:24:56.088622Z","shell.execute_reply":"2024-06-01T14:24:56.099469Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"zEE4JxeUthCb"}},{"cell_type":"markdown","source":"## Initialize model","metadata":{"id":"bIxII4QoticA"}},{"cell_type":"code","source":"import torch.optim as optim\n\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\noptimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)","metadata":{"id":"hdNsgxMG-Wwu","execution":{"iopub.status.busy":"2024-06-01T14:24:56.101463Z","iopub.execute_input":"2024-06-01T14:24:56.102201Z","iopub.status.idle":"2024-06-01T14:24:57.590087Z","shell.execute_reply.started":"2024-06-01T14:24:56.102176Z","shell.execute_reply":"2024-06-01T14:24:57.589217Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Training and Validating","metadata":{"id":"T98r4qZuuFyn"}},{"cell_type":"code","source":"def generate_train_val(train_pro=0.9):\n    real_train_size = int(train_pro*train_size)\n    val_size = train_size-real_train_size\n\n    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n    idx_train.sort()\n    idx_val = []\n    pointer = 0\n    for v in range(train_size):\n        if pointer<len(idx_train) and idx_train[pointer] == v:\n            pointer +=1\n        else:\n            idx_val.append(v)\n    idx_test = range(train_size+vocab_length, node_size)\n    return idx_train, idx_val, idx_test\n\nidx_train, idx_val, idx_test = generate_train_val()","metadata":{"id":"Bv9br9pgGw9R","execution":{"iopub.status.busy":"2024-06-01T14:24:57.591260Z","iopub.execute_input":"2024-06-01T14:24:57.591607Z","iopub.status.idle":"2024-06-01T14:24:57.603729Z","shell.execute_reply.started":"2024-06-01T14:24:57.591574Z","shell.execute_reply":"2024-06-01T14:24:57.602874Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef train_model(show_result = True):\n    val_loss = []\n    for epoch in range(NUM_EPOCHS):\n        t = time.time()\n        model.train()\n        optimizer.zero_grad()\n        output= model(features, adj)\n        loss_train = criterion(output[idx_train], labels[idx_train])\n        acc_train = cal_accuracy(output[idx_train], labels[idx_train])\n        loss_train.backward()\n        optimizer.step()\n\n        model.eval()\n        output = model(features, adj)\n\n        loss_val = criterion(output[idx_val], labels[idx_val])\n        val_loss.append(loss_val.item())\n        acc_val = cal_accuracy(output[idx_val], labels[idx_val])\n        if show_result:\n            print(  'Epoch: {:04d}'.format(epoch+1),\n                    'loss_train: {:.4f}'.format(loss_train.item()),\n                    'acc_train: {:.4f}'.format(acc_train),\n                    'loss_val: {:.4f}'.format(loss_val.item()),\n                    'acc_val: {:.4f}'.format(acc_val),\n                    'time: {:.4f}s'.format(time.time() - t))\n        \n        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n            if show_result:\n                print(\"Early Stopping...\")\n            break\n\ntrain_model()","metadata":{"id":"QC7u3Jn2uIu4","execution":{"iopub.status.busy":"2024-06-01T14:24:57.605020Z","iopub.execute_input":"2024-06-01T14:24:57.605273Z","iopub.status.idle":"2024-06-01T14:25:00.527671Z","shell.execute_reply.started":"2024-06-01T14:24:57.605251Z","shell.execute_reply":"2024-06-01T14:25:00.526598Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch: 0001 loss_train: 0.6931 acc_train: 0.4901 loss_val: 0.6917 acc_val: 0.4813 time: 1.0378s\nEpoch: 0002 loss_train: 0.6891 acc_train: 0.5024 loss_val: 0.6845 acc_val: 0.6552 time: 0.0759s\nEpoch: 0003 loss_train: 0.6820 acc_train: 0.6899 loss_val: 0.6746 acc_val: 0.7740 time: 0.0752s\nEpoch: 0004 loss_train: 0.6674 acc_train: 0.8405 loss_val: 0.6608 acc_val: 0.7573 time: 0.0722s\nEpoch: 0005 loss_train: 0.6473 acc_train: 0.8197 loss_val: 0.6415 acc_val: 0.7844 time: 0.0717s\nEpoch: 0006 loss_train: 0.6210 acc_train: 0.8442 loss_val: 0.6197 acc_val: 0.7896 time: 0.0714s\nEpoch: 0007 loss_train: 0.5926 acc_train: 0.8369 loss_val: 0.5969 acc_val: 0.7792 time: 0.0712s\nEpoch: 0008 loss_train: 0.5597 acc_train: 0.8484 loss_val: 0.5739 acc_val: 0.7917 time: 0.0714s\nEpoch: 0009 loss_train: 0.5262 acc_train: 0.8532 loss_val: 0.5488 acc_val: 0.7917 time: 0.0712s\nEpoch: 0010 loss_train: 0.4902 acc_train: 0.8580 loss_val: 0.5240 acc_val: 0.7885 time: 0.0712s\nEpoch: 0011 loss_train: 0.4544 acc_train: 0.8593 loss_val: 0.5030 acc_val: 0.7990 time: 0.0714s\nEpoch: 0012 loss_train: 0.4206 acc_train: 0.8654 loss_val: 0.4868 acc_val: 0.7937 time: 0.0712s\nEpoch: 0013 loss_train: 0.3913 acc_train: 0.8686 loss_val: 0.4701 acc_val: 0.7990 time: 0.0712s\nEpoch: 0014 loss_train: 0.3621 acc_train: 0.8716 loss_val: 0.4598 acc_val: 0.8010 time: 0.0714s\nEpoch: 0015 loss_train: 0.3373 acc_train: 0.8803 loss_val: 0.4556 acc_val: 0.8042 time: 0.0715s\nEpoch: 0016 loss_train: 0.3151 acc_train: 0.8819 loss_val: 0.4503 acc_val: 0.8063 time: 0.0734s\nEpoch: 0017 loss_train: 0.2948 acc_train: 0.8866 loss_val: 0.4498 acc_val: 0.8042 time: 0.0715s\nEpoch: 0018 loss_train: 0.2779 acc_train: 0.8914 loss_val: 0.4544 acc_val: 0.8021 time: 0.0712s\nEpoch: 0019 loss_train: 0.2624 acc_train: 0.9001 loss_val: 0.4579 acc_val: 0.8031 time: 0.0716s\nEpoch: 0020 loss_train: 0.2484 acc_train: 0.8990 loss_val: 0.4630 acc_val: 0.8031 time: 0.0713s\nEpoch: 0021 loss_train: 0.2352 acc_train: 0.9071 loss_val: 0.4753 acc_val: 0.8021 time: 0.0712s\nEpoch: 0022 loss_train: 0.2234 acc_train: 0.9112 loss_val: 0.4814 acc_val: 0.8052 time: 0.0712s\nEpoch: 0023 loss_train: 0.2104 acc_train: 0.9188 loss_val: 0.4894 acc_val: 0.8083 time: 0.0718s\nEpoch: 0024 loss_train: 0.2007 acc_train: 0.9214 loss_val: 0.5012 acc_val: 0.8135 time: 0.0714s\nEpoch: 0025 loss_train: 0.1910 acc_train: 0.9238 loss_val: 0.5208 acc_val: 0.8010 time: 0.0719s\nEpoch: 0026 loss_train: 0.1835 acc_train: 0.9299 loss_val: 0.5228 acc_val: 0.8135 time: 0.0718s\nEpoch: 0027 loss_train: 0.1735 acc_train: 0.9348 loss_val: 0.5345 acc_val: 0.8135 time: 0.0716s\nEarly Stopping...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"OQwlWq6dyYJm"}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\ndef test():\n    model.eval()\n    output = model(features, adj)\n    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n    acc = accuracy_score(test_labels,predictions)\n    f11 = f1_score(test_labels,predictions, average='macro')\n    f12 = f1_score(test_labels,predictions, average = 'weighted')\n    return acc, f11, f12\n\nprint(test())","metadata":{"id":"jmPNukmk40gd","execution":{"iopub.status.busy":"2024-06-01T14:25:00.529174Z","iopub.execute_input":"2024-06-01T14:25:00.529592Z","iopub.status.idle":"2024-06-01T14:25:00.564458Z","shell.execute_reply.started":"2024-06-01T14:25:00.529554Z","shell.execute_reply":"2024-06-01T14:25:00.563538Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"(0.7731958762886598, 0.7731622039496802, 0.7731492530500728)\n","output_type":"stream"}]}]}