{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100},{"sourceId":8053014,"sourceType":"datasetVersion","datasetId":4741673}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hierarchical Attention Networks for Document Classification\nPyTorch implementation of **Hierarchical Attention Networks for Document Classification (NAACL 2016)**\n\n# Table of Contents\n* [Preamble](#Preamble)\n* [Word2Vec Module](#Word2Vec-Module)\n* [Load Vocabulary and Embeddings](#Load-Vocabulary-and-Embeddings)\n* [PyTorch Dataset class](#PyTorch-Dataset-class)\n* [HAN Model](#HAN-Model)\n* [Training](#Training)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-07T04:41:24.876742Z","iopub.execute_input":"2024-04-07T04:41:24.877131Z","iopub.status.idle":"2024-04-07T04:41:24.883841Z","shell.execute_reply.started":"2024-04-07T04:41:24.877104Z","shell.execute_reply":"2024-04-07T04:41:24.882762Z"}}},{"cell_type":"markdown","source":"# Preamble","metadata":{}},{"cell_type":"code","source":"# Preamble\nimport time, random\nimport re, string\nimport os, sys\nimport math\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 64","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word2Vec Module\nFollowing the original paper, we trained the word embeddings using *Negative sampling* technique introduced in **Distributed Representations of Words and Phrases and their Compositionality (NIPS 2013)**","metadata":{}},{"cell_type":"code","source":"# Negative sampling embedding module\nclass NegSamplingEmbedding(nn.Module):\n    '''\n    Vocab_size: V\n    Embedding_size: E\n    Text_length: L\n    Batch_size: B\n    \n    Consult: https://github.com/mindspore-courses/DeepNLP-models-MindSpore/\n            blob/main/notebooks/02.Skip-gram-Negative-Sampling.ipynb\n    '''\n    def __init__(self, vocab_size, embedding_size):\n        super(NegSamplingEmbedding, self).__init__()\n        self.U = nn.Embedding(vocab_size, embedding_size) # Center embedding\n        self.V = nn.Embedding(vocab_size, embedding_size) # Outside embedding\n        self.LogSig = nn.LogSigmoid()\n        \n    def forward(self, wc, wo, wk, mask_c, mask_o, check_shape= False):\n        vc = self.V(wc) # Center embedding. Shape: (B, L, E)\n        uo = self.U(wo) # Outside embedding. Shape: (B, L, C, E)\n        uk = self.U(wk) # Random embedding. Shape: (B, L, C, E, K)\n        \n        if check_shape:\n            B = uk.shape[0]\n            L = uk.shape[1]\n            C = uk.shape[2]\n            K = uk.shape[3]\n            E = uk.shape[4]\n            print(f\"Basic shapes: B = {B}; L = {L}; C = {C}; K = {K}; E = {E}\")\n            print('*********************************')\n            print('Shape of vc:', vc.shape)\n            print('Shape of uo:', uo.shape)\n            print('Shape of uk:', uk.shape)\n            print('*********************************')\n        cmp1 = torch.einsum('blce,ble->blc', uo, vc) # Shape: (B, L, C)\n        cmp2 = torch.einsum('blcke,ble->blck', uk, vc) # Shape: (B, L, C, K)\n        \n        cmp1 = self.LogSig(cmp1) * mask_o # Shape: (B, L, C)\n        cmp2 = self.LogSig(-cmp2) # Shape: (B, L, C, K)\n        cmp2 = torch.einsum('blck->blc', cmp2) * mask_o # Shape: (B, L, C)\n    \n        cmp1 = torch.einsum('blc->bl', cmp1) # Shape: (B, L)\n        cmp2 = torch.einsum('blc->bl', cmp2) # Shape: (B, L)\n        \n        loss = torch.mean(cmp1 + cmp2)\n        \n        if check_shape:\n            print('Shape of cmp1:', cmp1.shape)\n            print('Shape of cmp2:', cmp2.shape)\n            print('Shape of LOSS:', loss.shape)\n        return -loss","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:24.885541Z","iopub.execute_input":"2024-04-07T04:41:24.885877Z","iopub.status.idle":"2024-04-07T04:41:24.899267Z","shell.execute_reply.started":"2024-04-07T04:41:24.885845Z","shell.execute_reply":"2024-04-07T04:41:24.898398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Vocabulary and Embeddings","metadata":{}},{"cell_type":"code","source":"# Prepare vocab, counter, tokenizer\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import vocab, build_vocab_from_iterator\n\nfrom collections import Counter\n\ntokenizer = get_tokenizer(\"basic_english\")\n\nvocab = torch.load('/kaggle/input/hlt-word2vec/vocab.pth')\ncounter = torch.load('/kaggle/input/hlt-word2vec/vocab.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:24.900885Z","iopub.execute_input":"2024-04-07T04:41:24.901153Z","iopub.status.idle":"2024-04-07T04:41:25.253067Z","shell.execute_reply.started":"2024-04-07T04:41:24.901114Z","shell.execute_reply":"2024-04-07T04:41:25.252070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Task configs\nVOCAB_SIZE = len(vocab)\nEMBEDDING_DIM = 200\nGRU_DIM = 50\nNUM_CLASSES = 5","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:25.254508Z","iopub.execute_input":"2024-04-07T04:41:25.254835Z","iopub.status.idle":"2024-04-07T04:41:25.259367Z","shell.execute_reply.started":"2024-04-07T04:41:25.254810Z","shell.execute_reply":"2024-04-07T04:41:25.258428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare word embeddings\nWORD2VEC_PATH = '/kaggle/input/hlt-word2vec/word2vec.pth'\nword2vec = NegSamplingEmbedding(VOCAB_SIZE, EMBEDDING_DIM)\nword2vec.load_state_dict(torch.load(WORD2VEC_PATH, map_location= DEVICE))\nword2vec.eval()\n\nW_e = word2vec.V.weight.detach().to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:25.261780Z","iopub.execute_input":"2024-04-07T04:41:25.262084Z","iopub.status.idle":"2024-04-07T04:41:25.743090Z","shell.execute_reply.started":"2024-04-07T04:41:25.262060Z","shell.execute_reply":"2024-04-07T04:41:25.742096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Dataset class","metadata":{}},{"cell_type":"code","source":"# YELP Dataset\ndef collate_batch(batch):\n    '''\n    Collate batch with zero-padding\n    Consult: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n    '''\n    texts = []\n    labels = []\n    mask = []\n    for _text, _label in batch:\n        texts.append(_text)\n        labels.append(_label)\n        \n    L = max([len(text) for text in texts])\n    \n    for i in range(len(texts)):\n        l = texts[i].shape[0]\n        cur_mask = torch.ones(L)\n        if l < L:\n            cur_mask[l:L] = 0\n            # Zero-padding text, only on one side.\n            texts[i] = F.pad(texts[i], (0, L-l), 'constant', 0)\n            \n        mask.append(cur_mask)\n    \n    texts = torch.stack(texts)\n    labels = torch.stack(labels)\n    mask = torch.stack(mask)\n    return texts, labels, mask\n\ndef collate_batch_HAN(batch):\n    return batch\n\nclass YELPDataset(Dataset):\n    def __init__(self, df, vocab, tokenizer, df_sort= True, punct_splt= False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n        self.len_vocab = len(vocab)\n        self.punct_splt = punct_splt\n        if df_sort:\n            self.sort_df_by_txt_len()\n    \n    def sort_df_by_txt_len(self):\n        len_list = [-len(self.df.iloc[i]['text']) for i in range(len(self.df))]\n        self.df = self.df.iloc[np.argsort(len_list)]\n    \n    def __len__(self):\n        return len(self.df)\n\n    def text_pipeline(self, x):\n        return self.vocab(self.tokenizer(x))\n\n    def label_pipeline(self, x):\n        return int(x) - 1\n    \n    def __getitem__(self, idx):\n        if not self.punct_splt:\n            txt = self.text_pipeline(self.df.iloc[idx]['text'])\n            txt = torch.tensor(txt, dtype= torch.int64)\n\n            label = self.label_pipeline(self.df.iloc[idx]['stars'])\n            label = torch.tensor(label, dtype= torch.int64)\n\n            return (txt, label)\n    \n        else:\n            txt = self.df.iloc[idx]['text']\n            sentences = re.split(\"[\" + string.punctuation + \"]+\", txt)\n            L = 0\n            X = []\n            mask = []\n            \n            for s in sentences:\n                l = len(s)\n                if l == 0:\n                    continue\n                L = max(L, l)\n                X.append(torch.tensor(self.text_pipeline(s), dtype= torch.int64))\n                \n            if len(X) == 0:\n                return (None, None, None)\n            \n            for i in range(len(X)):\n                l = X[i].shape[0]\n                cur_mask = torch.ones(L)\n                \n                # Zero-padding sentence\n                if(l < L):\n                    # Zero-padding sentence, only on one side.\n                    X[i] = F.pad(X[i], (0, L - l), 'constant', 0)\n                    cur_mask[l:L] = 0\n                \n                mask.append(cur_mask)\n                    \n            X = torch.stack(X)\n            mask = torch.stack(mask)\n            \n            label = self.label_pipeline(self.df.iloc[idx]['stars'])\n            label = torch.tensor(label, dtype= torch.int64)\n            \n            return (X, label, mask)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:23:56.337118Z","iopub.execute_input":"2024-04-07T06:23:56.338014Z","iopub.status.idle":"2024-04-07T06:23:56.357258Z","shell.execute_reply.started":"2024-04-07T06:23:56.337978Z","shell.execute_reply":"2024-04-07T06:23:56.356374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CSV Preparation\ndata_file = open(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\")\ndata = []\n\ncnt = 1569264 # Size of YELP 2015 dataset\n# cnt = 10000\n\nfor line in data_file:\n    data.append(json.loads(line))\n    cnt -= 1\n    if cnt == 0:\n        break\n    \ndata_file.close()\ndf = pd.DataFrame(data)\n\nprint(\"Number of datapoints:\", len(df))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:25.764730Z","iopub.execute_input":"2024-04-07T04:41:25.765063Z","iopub.status.idle":"2024-04-07T04:41:46.778877Z","shell.execute_reply.started":"2024-04-07T04:41:25.765022Z","shell.execute_reply":"2024-04-07T04:41:46.777797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-val-test splits\ndf_size = len(df)\nidx = [x for x in range(df_size)]\nrandom.Random(555).shuffle(idx)\n\ntrain_num = int(df_size * 0.8)\nval_num = int(df_size * 0.01)\ntest_num = int(df_size * 0.1)\n\n# print(train_num, val_num, test_num)\n\ntrain_idx = idx[:train_num]\nval_idx = idx[train_num : (train_num + val_num)]\ntest_idx = idx[(train_num + val_num) : ]\n\ntrain_df = df.iloc[train_idx]\nval_df = df.iloc[val_idx]\ntest_df = df.iloc[test_idx]\n\nprint('Size of trainset:', len(train_df))\nprint('Size of valset:', len(val_df))\nprint('Size of testset:', len(test_df))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:41:46.780154Z","iopub.execute_input":"2024-04-07T04:41:46.780410Z","iopub.status.idle":"2024-04-07T04:41:49.607669Z","shell.execute_reply.started":"2024-04-07T04:41:46.780389Z","shell.execute_reply":"2024-04-07T04:41:49.606753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset, Dataloader\ntrainset = YELPDataset(train_df, vocab, tokenizer, punct_splt= True)\nvalset = YELPDataset(val_df, vocab, tokenizer, punct_splt= True)\ntestset = YELPDataset(test_df, vocab, tokenizer, punct_splt= True)\n\ntrainloader = DataLoader(trainset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch_HAN)\nvalloader = DataLoader(valset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch_HAN)\ntestloader = DataLoader(testset, batch_size= BATCH_SIZE, \n                         shuffle= False, pin_memory= True, collate_fn= collate_batch_HAN)\n\nfor batch in trainloader:\n    X, y, mask = batch[0]\n    print(\"Shape of Texts:\", X.shape)\n    print(\"Shape of Labels:\", y.shape)\n    print(\"Shape of Mask:\", mask.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:26:10.914351Z","iopub.execute_input":"2024-04-07T06:26:10.914681Z","iopub.status.idle":"2024-04-07T06:27:48.819539Z","shell.execute_reply.started":"2024-04-07T06:26:10.914653Z","shell.execute_reply":"2024-04-07T06:27:48.818663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HAN Model","metadata":{}},{"cell_type":"code","source":"class HANModel(nn.Module):\n    def __init__(self, W_e, embedding_dim, gru_dim, \n                 num_classes):\n        super(HANModel, self).__init__()\n        \n        DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize\n        self.E = embedding_dim\n        self.G = gru_dim\n        self.num_classes = num_classes\n        \n        # Pretrained word embeddings\n        self.W_e = W_e\n        \n        # Word-level attention\n        self.WordEncoder = nn.GRU(\n            input_size= embedding_dim,\n            hidden_size= gru_dim,\n            batch_first= True,\n            bidirectional= True\n        )\n        self.WordMLP = nn.Sequential(\n            nn.Linear(2*gru_dim, 2*gru_dim),\n            nn.Tanh()\n        )\n        self.u_w = nn.Parameter(torch.randn(2*gru_dim))\n        \n        # Sequence-level attention\n        self.SeqEncoder = nn.GRU(\n            input_size= 2*gru_dim,\n            hidden_size= gru_dim,\n            batch_first= True,\n            bidirectional= True\n        )\n        self.SeqMLP = nn.Sequential(\n            nn.Linear(2*gru_dim, 2*gru_dim),\n            nn.Tanh()\n        )\n        self.u_s = nn.Parameter(torch.randn(2*gru_dim))\n        \n        # Classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(2*gru_dim, num_classes),\n            nn.Softmax()\n        )\n        \n    def forward(self, text, mask, check_shape= False):\n        '''\n        <text> is expected to have shape (S, L)\n        \n        Number of sentence: S\n        Max sentence length: L\n        Embedding size: E\n        GRU dimension: G -> Bidirectional: 2G\n        '''\n        S = text.shape[0]\n        L = text.shape[1]\n        \n        x = self.W_e[text] # Text embedding, Shape: (S, L, E)\n        hW,_ = self.WordEncoder(x) # Word annotation. Shape: (S, L, 2G)\n        uW = self.WordMLP(hW) # Word hidden representation. Shape: (S, L, 2G)\n\n        # Attention weight - Needs verifications\n        alphaW = nn.Softmax()(torch.einsum('slg,g->sl', uW, self.u_w) * mask) # Shape: (S, L)\n        \n        # Sentence vector\n        s = torch.einsum('slg,sl->sg', hW, alphaW) # Shape: (S, 2G)\n        hS,_ = self.SeqEncoder(s) # Sentence annotation. Shape: (S, 2G)\n        uS = self.SeqMLP(hS) # Sentence hidden representation. Shape: (S, 2G)\n        \n        # Attention weight - Needs verifications\n        alphaS = nn.Softmax()(torch.matmul(uS, self.u_s)) # Shape: (S)\n        \n        # Document vector\n        v = torch.matmul(alphaS, hS)\n        \n        logits = self.classifier(v)\n        \n        if check_shape:\n            print(f'Basic shapes: S = {S}, E = {self.E}, L = {L}, G = {self.G}')\n            print('*********************************')\n            print('Shape of x:', x.shape)\n            print('Shape of hW:', hW.shape)\n            print('Shape of uW:', uW.shape)\n            print('Shape of alphaW:', alphaW.shape)\n            print('*********************************')\n            print('Shape of s:', s.shape)\n            print('Shape of hS:', hS.shape)\n            print('Shape of uS:', uS.shape)\n            print('Shape of alphaS:', alphaS.shape)\n            print('*********************************')\n            print('Shape of v:', v.shape)\n            print('Shape of logits:', logits.shape)\n        \n        return logits\n        ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:43:27.032721Z","iopub.execute_input":"2024-04-07T04:43:27.033079Z","iopub.status.idle":"2024-04-07T04:43:27.048915Z","shell.execute_reply.started":"2024-04-07T04:43:27.033045Z","shell.execute_reply":"2024-04-07T04:43:27.047966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = HANModel(W_e, EMBEDDING_DIM, GRU_DIM, NUM_CLASSES).to(DEVICE)\n\nfor batch in trainloader:\n    X, y, mask = batch[0]\n    X, mask = X.to(DEVICE), mask.to(DEVICE)\n    logits = model(X, mask, check_shape= True)\n    break\n    \nprint('*********************************')\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:43:27.051585Z","iopub.execute_input":"2024-04-07T04:43:27.052048Z","iopub.status.idle":"2024-04-07T04:43:27.501435Z","shell.execute_reply.started":"2024-04-07T04:43:27.052014Z","shell.execute_reply":"2024-04-07T04:43:27.500443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Training configs\nLR = 1e-3\nMOMENTUM = 0.9\nEPOCHS = 3\nITER = EPOCHS * len(trainloader)\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr= LR)\n# OPTIMIZER = torch.optim.SGD(model.parameters(), lr= LR, momentum= MOMENTUM, nesterov= True)\nSCHEDULER = lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = ITER)\nLOSS_FN = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:43:27.502602Z","iopub.execute_input":"2024-04-07T04:43:27.502903Z","iopub.status.idle":"2024-04-07T04:43:27.508648Z","shell.execute_reply.started":"2024-04-07T04:43:27.502877Z","shell.execute_reply":"2024-04-07T04:43:27.507773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train procedures\ndef test(testloader, model, loss_fn):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    bcnt = 0\n    cnt = 0\n    \n    for i, batch in enumerate(testloader):\n        tmp_loss = 0\n        cnt += 1\n        for (X, y, mask) in batch:\n            if X is None:\n                continue\n            bcnt += 1\n            X, y, mask = X.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n            pred = model(X, mask)\n            tmp_loss += loss_fn(logits, y).item()\n            correct += (pred.argmax(0) == y).type(torch.float).sum().item()\n        tmp_loss /= len(batch)\n        test_loss += tmp_loss\n        \n    test_loss /= len(testloader)\n    accuracy = correct / len(testloader.dataset)\n    \n    return test_loss, accuracy\n\ndef train(trainloader, valloader, model, optimizer, scheduler, loss_fn, val_freq):\n    model.train()\n    tloss = []\n    cur_acc = 0\n    for i, batch in enumerate(trainloader):\n        loss = 0\n        for (X, y, mask) in batch:\n            if X is None:\n                continue\n            X, y, mask = X.to(DEVICE), y.to(DEVICE), mask.to(DEVICE)\n            logits = model(X, mask)\n            loss += loss_fn(logits, y)\n        loss /= len(batch)\n        \n        if val_freq > 0 and i % val_freq == 0:\n            tloss.append(loss.cpu().detach().numpy())\n            model.eval()\n            val_loss, val_acc = test(valloader, model, loss_fn)\n            model.train()\n            print(f'Iter {i}, loss = {tloss[-1]}, val_acc = {val_acc}')\n            if cur_acc < val_acc:\n                cur_acc = val_acc\n                print('Saving model...')\n                torch.save(model.state_dict(), f'HAN_{val_acc*100}.pth')\n        \n        tloss.append(loss.cpu().detach().numpy())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n    model.eval()\n    val_loss, val_acc = test(valloader, model, loss_fn)\n    model.train()\n    print(f'Iter {i}, loss = {tloss[-1]}, val_acc = {val_acc}')\n    if cur_acc < val_acc:\n        cur_acc = val_acc\n        print('Saving model...')\n        torch.save(model.state_dict(), f'HAN_{val_acc*100}.pth')\n        \n    return tloss","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:33:15.599764Z","iopub.execute_input":"2024-04-07T06:33:15.600372Z","iopub.status.idle":"2024-04-07T06:33:15.614797Z","shell.execute_reply.started":"2024-04-07T06:33:15.600345Z","shell.execute_reply":"2024-04-07T06:33:15.613832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TRAINING\niter_loss = []\nepoch_loss = []\nbest_acc = 0\n\nfor t in range(EPOCHS):\n    print(f'Epoch {t} starts.')\n    tloss = train(trainloader, valloader, model, OPTIMIZER, SCHEDULER, LOSS_FN, 1000)\n    val_loss, val_acc = test(valloader, model, LOSS_FN)\n    \n    iter_loss = iter_loss + tloss\n    epoch_loss.append(sum(tloss) / len(tloss))\n    \n    print(f'Epoch {t}: LOSS = {epoch_loss[-1]}, VAL-ACC = {val_acc}')\n    \nfig, axes = plt.subplots()\naxes.plot(iter_loss, label = 'train-loss')\naxes.legend()\naxes.set_xlabel('Iteration')\naxes.set_ylabel('Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T04:43:27.524921Z","iopub.execute_input":"2024-04-07T04:43:27.525180Z","iopub.status.idle":"2024-04-07T05:44:01.155872Z","shell.execute_reply.started":"2024-04-07T04:43:27.525157Z","shell.execute_reply":"2024-04-07T05:44:01.154587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, val_acc = test(testloader, model, LOSS_FN)\nprint(f'Test accuracy: {val_acc}')","metadata":{"execution":{"iopub.status.busy":"2024-04-07T06:33:20.746161Z","iopub.execute_input":"2024-04-07T06:33:20.746493Z","iopub.status.idle":"2024-04-07T06:47:55.104249Z","shell.execute_reply.started":"2024-04-07T06:33:20.746469Z","shell.execute_reply":"2024-04-07T06:47:55.103285Z"},"trusted":true},"execution_count":null,"outputs":[]}]}