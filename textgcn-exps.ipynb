{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100},{"sourceId":8053014,"sourceType":"datasetVersion","datasetId":4741673},{"sourceId":8528311,"sourceType":"datasetVersion","datasetId":5093015},{"sourceId":180945714,"sourceType":"kernelVersion"}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GCN for Text Classification\nPyTorch reimplementation of **Graph Convolutional Networks for Text Classification (AAAI 2019)**\n\n# Table of Content\n* [Preamble](#Preamble)\n* [Data preparation](#Data-preparation)\n    * [Load raw data](#Load-raw-data)\n    * [Train-test split](#Train-test-split)\n    * [Setup vocabulary utils](#Setup-vocabulary-utils)\n    * [Setup statistics utils](#Setup-statistics-utils)\n    * [Build Text graph](#Build-Text-graph)\n* [PyTorch Dataset class](#PyTorch-Dataset-class)\n* [TextGCN Module](#TextGCN-Module)\n* [Training](#Training)\n    * [Prepare text graphs](#Prepare-text-graphs)\n    * [Training configs](#Training-configs)\n    * [Verify training accuracy](#Verify-training-accuracy)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T03:38:16.134645Z","iopub.execute_input":"2024-04-10T03:38:16.135238Z","iopub.status.idle":"2024-04-10T03:38:24.016851Z","shell.execute_reply.started":"2024-04-10T03:38:16.135211Z","shell.execute_reply":"2024-04-10T03:38:24.015998Z"}}},{"cell_type":"markdown","source":"# Preamble","metadata":{}},{"cell_type":"code","source":"# Preamble\nimport time, random\nimport re, string\nimport os, sys\nimport math\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom tqdm import tqdm\n\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nCPU = torch.device(\"cpu\")\nWINDOW_SIZE = 20\nEMBEDDING_SIZE = 200\nNUM_CLASSES = 2\nDROPOUT = 0.5","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.416450Z","iopub.execute_input":"2024-06-03T01:26:04.417583Z","iopub.status.idle":"2024-06-03T01:26:04.425469Z","shell.execute_reply.started":"2024-06-03T01:26:04.417542Z","shell.execute_reply":"2024-06-03T01:26:04.423869Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation\nThe word embeddings is trained on YELP-review dataset: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset","metadata":{}},{"cell_type":"markdown","source":"## Load raw data","metadata":{}},{"cell_type":"code","source":"# # YELP Preparation\n# data_file = open(\"/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json\")\n# data = []\n\n# # cnt = 1569264 # Size of YELP 2015 dataset\n# cnt = 100000\n\n# for line in data_file:\n#     data.append(json.loads(line))\n#     cnt -= 1\n#     if cnt == 0:\n#         break\n    \n# data_file.close()\n# df = pd.DataFrame(data)\n\n# print(\"Number of datapoints:\", len(df))\n# df.head()\n\n# # R52 Preparation\n# train_df = pd.read_csv('/kaggle/input/smolcsv/r52-train-stemmed.csv')\n# val_df = pd.read_csv('/kaggle/input/smolcsv/r52-dev-stemmed.csv')\n# test_df = pd.read_csv('/kaggle/input/smolcsv/r52-test-stemmed.csv')\n\n# df = pd.concat([train_df, val_df, test_df])\n\n# print('Size of data corpus:', len(df))\n# df.head()\n\n# MR Preparation\ndf = pd.read_csv('/kaggle/input/smolcsv/MR.csv')\n\nprint(\"Number of datapoints:\", len(df))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.427836Z","iopub.execute_input":"2024-06-03T01:26:04.428280Z","iopub.status.idle":"2024-06-03T01:26:04.490739Z","shell.execute_reply.started":"2024-06-03T01:26:04.428243Z","shell.execute_reply":"2024-06-03T01:26:04.489730Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Number of datapoints: 10662\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text Sentiment  \\\n0           0  the rock is destined to be the 21st century's ...  positive   \n1           1  the gorgeously elaborate continuation of \" the...  positive   \n2           2                     effective but too-tepid biopic  positive   \n3           3  if you sometimes like to go to the movies to h...  positive   \n4           4  emerges as something rare , an issue movie tha...  positive   \n\n   label  \n0      1  \n1      1  \n2      1  \n3      1  \n4      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>Sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>the rock is destined to be the 21st century's ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>the gorgeously elaborate continuation of \" the...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>effective but too-tepid biopic</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>if you sometimes like to go to the movies to h...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>emerges as something rare , an issue movie tha...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Train-test split\nOnly trainset is needed for building pretrained **TextGCN**.","metadata":{}},{"cell_type":"markdown","source":"## Setup vocabulary utils\nHere the frequency counter, vocab object and tokenizer is defined.\n\nReference: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html and https://github.com/usydnlp/TextGCN_analysis/blob/main/final.ipynb","metadata":{}},{"cell_type":"code","source":"# MR - YELP preparation\ndf_size = len(df)\nidx = [x for x in range(df_size)]\nrandom.Random(555).shuffle(idx)\n\ntrain_num = int(df_size * 0.8)\nval_num = int(df_size * 0.1)\ntest_num = int(df_size * 0.1)\n\n# print(train_num, val_num, test_num)\n\ntrain_idx = idx[:train_num]\nval_idx = idx[train_num : (train_num + val_num)]\ntest_idx = idx[(train_num + val_num) : ]\n\ntrain_df = df.iloc[train_idx].reset_index()\nval_df = df.iloc[val_idx].reset_index()\ntest_df = df.iloc[test_idx].reset_index()\n\n# # R52 Preparation\n# train_df = pd.read_csv('/kaggle/input/smolcsv/r52-train-stemmed.csv')\n# val_df = pd.read_csv('/kaggle/input/smolcsv/r52-dev-stemmed.csv')\n# test_df = pd.read_csv('/kaggle/input/smolcsv/r52-test-stemmed.csv')\n\n# print('Size of trainset:', len(train_df))\n# print('Size of valset:', len(val_df))\n# print('Size of testset:', len(test_df))\n\ndf = pd.concat([train_df, val_df, test_df])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.492155Z","iopub.execute_input":"2024-06-03T01:26:04.492480Z","iopub.status.idle":"2024-06-03T01:26:04.514603Z","shell.execute_reply.started":"2024-06-03T01:26:04.492453Z","shell.execute_reply":"2024-06-03T01:26:04.513485Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n\ndef clean_str(string, vocab_wordlist):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    string = string.strip().lower()\n    \n# #   Remove stop words\n#     words = string.split()\n#     new_words = []\n#     for word in words:\n#         if word not in stop_words and word in vocab_wordlist.keys() and vocab_wordlist[word] >= 5:\n#             new_words.append(word)\n            \n#     return ' '.join(new_words)\n\n    return string","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.517245Z","iopub.execute_input":"2024-06-03T01:26:04.517610Z","iopub.status.idle":"2024-06-03T01:26:04.528620Z","shell.execute_reply.started":"2024-06-03T01:26:04.517578Z","shell.execute_reply":"2024-06-03T01:26:04.527618Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set up Vocab\n# Source: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import vocab, build_vocab_from_iterator\n\nfrom collections import Counter\n\ntokenizer = get_tokenizer(\"basic_english\")\n\ndef get_counter(texts):\n    counter = Counter()\n    for text in texts:\n        counter.update(tokenizer(text))\n    return counter\n\ndef get_vocab(texts, unk= False):\n    counter = get_counter(texts)\n    if unk:\n        vocabulary = vocab(\n            counter,\n            specials= [\"<unk>\"],\n            min_freq= 1\n        )\n        vocabulary.set_default_index(vocabulary[\"<unk>\"])\n    else:\n        vocabulary = vocab(\n        counter,\n        min_freq= 1\n    )\n    return vocabulary\n\ndef text_pipeline(text, vocabulary):\n    return vocabulary(tokenizer(text))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.531054Z","iopub.execute_input":"2024-06-03T01:26:04.531509Z","iopub.status.idle":"2024-06-03T01:26:04.541782Z","shell.execute_reply.started":"2024-06-03T01:26:04.531480Z","shell.execute_reply":"2024-06-03T01:26:04.540653Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Clean data\ncorpus = []\nfor i in range(len(train_df)):\n    corpus.append(train_df.iloc[i]['text'])\nfor i in range(len(val_df)):\n    corpus.append(val_df.iloc[i]['text'])\nfor i in range(len(test_df)):\n    corpus.append(test_df.iloc[i]['text'])\n    \npre_wordlist = get_counter(corpus)\n\nfor i in range(len(train_df)):\n    s = train_df.iloc[i]['text']\n    train_df.at[i, 'text'] = clean_str(s, pre_wordlist)\nfor i in range(len(val_df)):\n    s = val_df.iloc[i]['text']\n    val_df.at[i, 'text'] = clean_str(s, pre_wordlist)\nfor i in range(len(test_df)):\n    s = test_df.iloc[i]['text']\n    test_df.at[i, 'text'] = clean_str(s, pre_wordlist)\n    \ndf = pd.concat([train_df, val_df, test_df])\n\nprint('Size of data corpus:', len(df))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:04.543101Z","iopub.execute_input":"2024-06-03T01:26:04.543432Z","iopub.status.idle":"2024-06-03T01:26:06.641354Z","shell.execute_reply.started":"2024-06-03T01:26:04.543405Z","shell.execute_reply":"2024-06-03T01:26:06.640295Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Size of data corpus: 10662\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setup statistics utils\nHere the statistics metrics $TF-IDF$ and $PMI$ are defined. Words in Vocab are indexed from $1\\dots N$ and documents are indexed from $1\\dots V$","metadata":{}},{"cell_type":"code","source":"import scipy.sparse as sp # Sparse matrix utilities\nfrom collections import defaultdict ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.642862Z","iopub.execute_input":"2024-06-03T01:26:06.643190Z","iopub.status.idle":"2024-06-03T01:26:06.648330Z","shell.execute_reply.started":"2024-06-03T01:26:06.643162Z","shell.execute_reply":"2024-06-03T01:26:06.646687Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import coo_matrix\n\n# Sparse matrix utils\ndef build_sparse(W_dict, shape):\n    W_row, W_col, W_data = [], [], []\n    for (current, nxt) in W_dict.keys():\n        W_row.append(current)\n        W_col.append(nxt)\n        W_data.append(W_dict[(current, nxt)])\n        \n    return sp.csr_matrix((W_data, (W_row, W_col)), shape= shape)\n\ndef ret_zero():\n    return 0\n\ndef csr2spMat(X_csr):\n    '''\n    Consult: https://stackoverflow.com/questions/50665141/converting-a-scipy-coo-matrix-to-pytorch-sparse-tensor\n    '''\n    X_coo = sp.csr_matrix.tocoo(X_csr, copy= True)\n    \n    values = X_coo.data\n    indices = np.vstack((X_coo.row, X_coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = X_coo.shape\n\n    return torch.sparse.FloatTensor(i, v, torch.Size(shape)).to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.649980Z","iopub.execute_input":"2024-06-03T01:26:06.650401Z","iopub.status.idle":"2024-06-03T01:26:06.659826Z","shell.execute_reply.started":"2024-06-03T01:26:06.650365Z","shell.execute_reply":"2024-06-03T01:26:06.658714Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Statistics\ndef get_W(texts, window_size):\n    '''\n    Get the number of windows over texts.\n    <texts> is expected to have type of list(torch.Tensor)\n    \n    Batch_size: B = len(texts)\n    '''\n    res = 0\n    for text in texts:\n        res += max(len(text) - window_size + 1, 1)\n    return res\n\n\ndef build_W(texts, vocabulary, window_size, check= False):\n    '''\n    Build the co-occurence matrix W which will be represented using csr_matrix((data, (row_ind, col_ind))\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Batch size: B = len(texts)\n    Vocab size: V = len(vocabulary)\n    Number of Node: N = B + V\n    \n    Output:\n    -----\n    W_dict (defaultdict) : Edge-list for construction of W\n    W (sp.csr_matrix): Co-occurence matrix. Shape = (V, V)\n    mW (np.ndarray): Occurence vector. Can be viewed as marginallized W. Shape = (V)\n    '''\n    B = len(texts)\n    V = len(vocabulary)\n    N = B + V\n    mW = np.zeros((V)) # Margin W - Occurence vector\n    \n    W_dict = defaultdict(ret_zero) \n    \n    if check:\n        print('*********************************')\n        print('Tokenized texts:')\n    for text in texts:\n        L = len(text)\n        if check:\n            print(text)\n        for i in range(max(L - window_size + 1, 1)):\n            master_range = text[i : min(i + window_size, L)]\n            mW[np.unique(master_range)] += 1\n            for current in master_range:\n                W_dict[(current, current)] = W_dict[(current, current)] + 1\n                for nxt in master_range:\n                    if current != nxt:\n                        W_dict[(current, nxt)] = W_dict[(current, nxt)] + 1\n                    \n    W = build_sparse(W_dict, shape= (V, V))\n\n    return W_dict, W, mW\n\ndef calc_pmi(texts, vocabulary, window_size):\n    '''\n    Calculate PMI\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    \n    Output:\n    -----\n    pmi_dict (defaultdict) : Edge-list for construction of pmi\n    pmi (sp.csr_matrix): Pairwise PMI matrix. Shape = (V, V)\n    '''\n    # Preparations\n    nW = get_W(texts, window_size) # Number of windows\n    W_dict, W, mW = build_W(texts, vocabulary, window_size) # W_ij. Shape: (V, V)\n    \n    V = len(vocabulary)\n    pmi_dict = defaultdict(ret_zero)\n    margin_p = (mW / nW)\n    \n    # Constructing PMI edge list.\n    for (i, j) in W_dict.keys():\n        if i == j:\n            continue\n        pij = 1.0 * W_dict[(i, j)] / nW\n        pi = margin_p[i]\n        pj = margin_p[j]\n        \n        prelog = 1.0 * pij / (pi * pj)\n        if prelog <= 1.0:\n            continue\n        pmi_dict[(i, j)] = math.log(prelog)\n        \n    pmi = build_sparse(pmi_dict, shape= (V, V))\n    \n    return pmi_dict, pmi\n    \n\ndef calc_tf_idf(texts, vocabulary):\n    '''\n    Calculate TF-IDFs\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    \n    NOTES:  For text graph constructions, words will be indexed from 0 to V-1 \n            while documents will be indexed from V to V + D - 1 \n    '''\n    V = len(vocabulary)\n    B = len(texts)\n    \n    tf_dict = defaultdict(ret_zero) # Term frequency - Shape: (B, V)\n    idf_dict = defaultdict(ret_zero) # Inverse document frequency - Shape: (V,)\n    tf_idf_dict = defaultdict(ret_zero) # TF - IDF - Shape: (B, V)\n    \n    idf = np.zeros((V)) # No. of documents containing token. The array is 0-based instead of V-based\n    \n    # Term frequency\n    for i, text in enumerate(texts):\n        L = text.shape[0]\n        for j in range(text.shape[0]):\n            tf_dict[(V + i, int(text[j]))] += 1\n            idf_dict[(V + i, int(text[j]))] = 1\n        # Normalize term frequency\n        for j in set(text.tolist()):\n            tf_dict[(V + i, j)] =  1.0 * tf_dict[(V + i, j)] / L\n    \n    # Inverse document frequency\n    for (i, j) in idf_dict.keys():\n        idf[j] += idf_dict[(i, j)]\n        \n#     idf[0] = B # There is no unknown token. Hence, idf[<unk>] = 0\n    idf = np.log(1.0 * B / idf) # Shape: (V,)\n    \n    # TF - IDF\n    for (i, j) in tf_dict.keys():\n        tmp = 1.0 * tf_dict[(i, j)] * idf[j]\n        tf_idf_dict[(i, j)] = tmp\n        tf_idf_dict[(j, i)] = tmp\n\n    tf = build_sparse(tf_dict, shape= (B + V, V))\n    tf_idf = build_sparse(tf_idf_dict, shape= (B + V, V + B))\n    \n    return tf_dict, tf, idf, tf_idf_dict, tf_idf","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.661646Z","iopub.execute_input":"2024-06-03T01:26:06.662103Z","iopub.status.idle":"2024-06-03T01:26:06.682845Z","shell.execute_reply.started":"2024-06-03T01:26:06.662066Z","shell.execute_reply":"2024-06-03T01:26:06.681617Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Pipeline testing\nraw_texts = ['A two-layer GCN can allow message', \n             'A two-layer GCN can allow message', \n             'A two-layer GCN can allow message',\n             'A two-layer GCN can allow message',\n             'A two-layer GCN can allow message',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'Thus although there is no direct document-document edges in the graph',\n             'In our preliminary experiment',\n             'In our preliminary experiment',\n             'In our preliminary experiment',\n             'In our preliminary experiment',\n             'In our preliminary experiment']\nvocabulary = get_vocab(raw_texts)\nprint('*********************************')\nprint('Token list:')\nprint(vocabulary.get_itos())\nprint('*********************************')\ntexts = []\nfor text in raw_texts:\n    texts.append(text_pipeline(text, vocabulary))\n    \nprint('TEXTS:')\nprint(texts)\n    \nprint('*********************************')\nprint('Vocabulary size V = ', len(vocabulary))\n_, W, mW = build_W(texts, vocabulary, WINDOW_SIZE, True)\nprint('*********************************')\nprint('Shape of W:', W.shape, 'Number of stored values:', W.nnz)\nprint('Shape of mW:', mW.shape)\nprint('*********************************')\nprint('Co-occurence matrix W:')\nprint(W)\nprint('*********************************')\nprint('Margin-W:')\nprint(mW)\nprint('*********************************')\nprint('PMI:')\n_, pmi = calc_pmi(texts, vocabulary, WINDOW_SIZE)\nprint('Shape of PMI', pmi.shape, 'Number of stored values:', pmi.nnz)\nprint(pmi)\nprint('*********************************')\nprint('TF-IDF:')\ntexts = [torch.Tensor(text) for text in texts]\n_,_,_,_, tf_idf = calc_tf_idf(texts, vocabulary)\nprint('Shape of TF-IDF', tf_idf.shape, 'Number of stored values:', tf_idf.nnz)\nprint(tf_idf)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.686505Z","iopub.execute_input":"2024-06-03T01:26:06.686890Z","iopub.status.idle":"2024-06-03T01:26:06.810510Z","shell.execute_reply.started":"2024-06-03T01:26:06.686859Z","shell.execute_reply":"2024-06-03T01:26:06.809390Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"*********************************\nToken list:\n['a', 'two-layer', 'gcn', 'can', 'allow', 'message', 'thus', 'although', 'there', 'is', 'no', 'direct', 'document-document', 'edges', 'in', 'the', 'graph', 'our', 'preliminary', 'experiment']\n*********************************\nTEXTS:\n[[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], [14, 17, 18, 19], [14, 17, 18, 19], [14, 17, 18, 19], [14, 17, 18, 19], [14, 17, 18, 19]]\n*********************************\nVocabulary size V =  20\n*********************************\nTokenized texts:\n[0, 1, 2, 3, 4, 5]\n[0, 1, 2, 3, 4, 5]\n[0, 1, 2, 3, 4, 5]\n[0, 1, 2, 3, 4, 5]\n[0, 1, 2, 3, 4, 5]\n[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n[14, 17, 18, 19]\n[14, 17, 18, 19]\n[14, 17, 18, 19]\n[14, 17, 18, 19]\n[14, 17, 18, 19]\n*********************************\nShape of W: (20, 20) Number of stored values: 172\nShape of mW: (20,)\n*********************************\nCo-occurence matrix W:\n  (0, 0)\t5\n  (0, 1)\t5\n  (0, 2)\t5\n  (0, 3)\t5\n  (0, 4)\t5\n  (0, 5)\t5\n  (1, 0)\t5\n  (1, 1)\t5\n  (1, 2)\t5\n  (1, 3)\t5\n  (1, 4)\t5\n  (1, 5)\t5\n  (2, 0)\t5\n  (2, 1)\t5\n  (2, 2)\t5\n  (2, 3)\t5\n  (2, 4)\t5\n  (2, 5)\t5\n  (3, 0)\t5\n  (3, 1)\t5\n  (3, 2)\t5\n  (3, 3)\t5\n  (3, 4)\t5\n  (3, 5)\t5\n  (4, 0)\t5\n  :\t:\n  (15, 15)\t5\n  (15, 16)\t5\n  (16, 6)\t5\n  (16, 7)\t5\n  (16, 8)\t5\n  (16, 9)\t5\n  (16, 10)\t5\n  (16, 11)\t5\n  (16, 12)\t5\n  (16, 13)\t5\n  (16, 14)\t5\n  (16, 15)\t5\n  (16, 16)\t5\n  (17, 14)\t5\n  (17, 17)\t5\n  (17, 18)\t5\n  (17, 19)\t5\n  (18, 14)\t5\n  (18, 17)\t5\n  (18, 18)\t5\n  (18, 19)\t5\n  (19, 14)\t5\n  (19, 17)\t5\n  (19, 18)\t5\n  (19, 19)\t5\n*********************************\nMargin-W:\n[ 5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5. 10.  5.  5.  5.\n  5.  5.]\n*********************************\nPMI:\nShape of PMI (20, 20) Number of stored values: 152\n  (0, 1)\t1.0986122886681098\n  (0, 2)\t1.0986122886681098\n  (0, 3)\t1.0986122886681098\n  (0, 4)\t1.0986122886681098\n  (0, 5)\t1.0986122886681098\n  (1, 0)\t1.0986122886681098\n  (1, 2)\t1.0986122886681098\n  (1, 3)\t1.0986122886681098\n  (1, 4)\t1.0986122886681098\n  (1, 5)\t1.0986122886681098\n  (2, 0)\t1.0986122886681098\n  (2, 1)\t1.0986122886681098\n  (2, 3)\t1.0986122886681098\n  (2, 4)\t1.0986122886681098\n  (2, 5)\t1.0986122886681098\n  (3, 0)\t1.0986122886681098\n  (3, 1)\t1.0986122886681098\n  (3, 2)\t1.0986122886681098\n  (3, 4)\t1.0986122886681098\n  (3, 5)\t1.0986122886681098\n  (4, 0)\t1.0986122886681098\n  (4, 1)\t1.0986122886681098\n  (4, 2)\t1.0986122886681098\n  (4, 3)\t1.0986122886681098\n  (4, 5)\t1.0986122886681098\n  :\t:\n  (15, 10)\t1.0986122886681098\n  (15, 11)\t1.0986122886681098\n  (15, 12)\t1.0986122886681098\n  (15, 13)\t1.0986122886681098\n  (15, 14)\t0.4054651081081644\n  (15, 16)\t1.0986122886681098\n  (16, 6)\t1.0986122886681098\n  (16, 7)\t1.0986122886681098\n  (16, 8)\t1.0986122886681098\n  (16, 9)\t1.0986122886681098\n  (16, 10)\t1.0986122886681098\n  (16, 11)\t1.0986122886681098\n  (16, 12)\t1.0986122886681098\n  (16, 13)\t1.0986122886681098\n  (16, 14)\t0.4054651081081644\n  (16, 15)\t1.0986122886681098\n  (17, 14)\t0.4054651081081644\n  (17, 18)\t1.0986122886681098\n  (17, 19)\t1.0986122886681098\n  (18, 14)\t0.4054651081081644\n  (18, 17)\t1.0986122886681098\n  (18, 19)\t1.0986122886681098\n  (19, 14)\t0.4054651081081644\n  (19, 17)\t1.0986122886681098\n  (19, 18)\t1.0986122886681098\n*********************************\nTF-IDF:\nShape of TF-IDF (35, 35) Number of stored values: 210\n  (0, 20)\t0.1831020481113516\n  (0, 21)\t0.1831020481113516\n  (0, 22)\t0.1831020481113516\n  (0, 23)\t0.1831020481113516\n  (0, 24)\t0.1831020481113516\n  (1, 20)\t0.1831020481113516\n  (1, 21)\t0.1831020481113516\n  (1, 22)\t0.1831020481113516\n  (1, 23)\t0.1831020481113516\n  (1, 24)\t0.1831020481113516\n  (2, 20)\t0.1831020481113516\n  (2, 21)\t0.1831020481113516\n  (2, 22)\t0.1831020481113516\n  (2, 23)\t0.1831020481113516\n  (2, 24)\t0.1831020481113516\n  (3, 20)\t0.1831020481113516\n  (3, 21)\t0.1831020481113516\n  (3, 22)\t0.1831020481113516\n  (3, 23)\t0.1831020481113516\n  (3, 24)\t0.1831020481113516\n  (4, 20)\t0.1831020481113516\n  (4, 21)\t0.1831020481113516\n  (4, 22)\t0.1831020481113516\n  (4, 23)\t0.1831020481113516\n  (4, 24)\t0.1831020481113516\n  :\t:\n  (29, 12)\t0.09987384442437362\n  (29, 13)\t0.09987384442437362\n  (29, 14)\t0.036860464373469494\n  (29, 15)\t0.09987384442437362\n  (29, 16)\t0.09987384442437362\n  (30, 14)\t0.1013662770270411\n  (30, 17)\t0.27465307216702745\n  (30, 18)\t0.27465307216702745\n  (30, 19)\t0.27465307216702745\n  (31, 14)\t0.1013662770270411\n  (31, 17)\t0.27465307216702745\n  (31, 18)\t0.27465307216702745\n  (31, 19)\t0.27465307216702745\n  (32, 14)\t0.1013662770270411\n  (32, 17)\t0.27465307216702745\n  (32, 18)\t0.27465307216702745\n  (32, 19)\t0.27465307216702745\n  (33, 14)\t0.1013662770270411\n  (33, 17)\t0.27465307216702745\n  (33, 18)\t0.27465307216702745\n  (33, 19)\t0.27465307216702745\n  (34, 14)\t0.1013662770270411\n  (34, 17)\t0.27465307216702745\n  (34, 18)\t0.27465307216702745\n  (34, 19)\t0.27465307216702745\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build Text graph\nThe adjacency matrix $A$ is designed as follow:\n\n\\begin{equation}\nA = \\begin{pmatrix}\nPMI & TF-IDF\\\\\nTF-IDF & 0\n\\end{pmatrix}\n\\end{equation}","metadata":{}},{"cell_type":"code","source":"def build_A(texts, vocabulary, window_size, nNodes):\n    '''\n    Calculate adjacency matrix A with shape (V+B, V+B)\n    <texts> is expected to have type of list(torch.Tensor)\n    <vocab> is the Vocabulary built over text\n    \n    Vocab size: V\n    Text length: L\n    Batch size: B\n    Nodes: N = B + V\n    \n    '''\n    _, _, _, tf_idf_dict, _ = calc_tf_idf(texts, vocabulary)\n    pmi_dict, _ = calc_pmi(texts, vocabulary, window_size)\n    \n    B = len(texts)\n    V = len(vocabulary)\n    \n    N = B + V # Number of nodes\n    \n    A_dict = defaultdict(ret_zero) # Adjacency matrix. Shape: (N, N)\n    \n    # Word-word edges, obtained by pmi_dict\n    for x in pmi_dict.keys():\n        A_dict[x] = pmi_dict[x]\n        \n    # Document-word edges, obtained by tf_idf_dict\n    for x in tf_idf_dict.keys():\n#         assert x[0] >= V and x[1] < V # Ensure edge (i, j) has i as document index, and j as token index\n        A_dict[x] = tf_idf_dict[x]\n    \n    if nNodes is None:\n        for i in range(N):\n            A_dict[(i, i)] = 1\n        A = build_sparse(A_dict, shape= (N, N))\n    else:\n        for i in range(nNodes):\n            A_dict[(i, i)] = 1\n        A = build_sparse(A_dict, shape= (nNodes, nNodes))\n    \n    return A_dict, A\n\ndef build_graph(texts, vocabulary, window_size, nNodes= None):\n    '''\n    Build A_tilde = D x A x D\n    \n    Nodes: N\n    '''\n    B = len(texts)\n    V = len(vocabulary)\n    \n    N = B + V # Number of nodes\n    \n    A_dict, A = build_A(texts, vocabulary, window_size, nNodes= nNodes)\n    \n    D_dict = defaultdict(ret_zero)\n    \n    for (i, j) in A_dict.keys():\n        D_dict[(i, i)] = D_dict[(i, i)] + A[(i, j)]\n    \n    if nNodes is None:\n        for i in range(N):\n            D_dict[(i, i)] = (D_dict[(i, i)] ** (-0.5))\n    else:\n        for i in range(nNodes):\n            D_dict[(i, i)] = ((1.0 * D_dict[(i, i)]) ** (-0.5))\n    \n    if nNodes is None:\n        D = build_sparse(D_dict, shape= (N, N)) # Degree matrix\n    else:\n        D = build_sparse(D_dict, shape= (nNodes, nNodes))\n    G = (D * A) * D # A_tilde. Shape = (N, N)\n    \n    return G\n\ndef build_X(vocab, grand_vocab, batch_size, pretrained= True, W_e= None, init_dim= None):\n    '''\n    Construct graph feature input\n    <vocab>: Vocabulary constructed by batch\n    <grand_vocab>: Vocabulary created by pretrained embedding\n    \n    Embedding size: E\n    Batch size: B\n    Number of nodes: N\n    \n    Output:\n    -----\n    X (torch.Tensor, pretrained= True) : Embedded feature matrix. Shape = (N, N)\n    X (sp.csr_matrix, pretrained= False): Sparse one-hot feature matrix. Shape = (V, V)\n    '''\n    if pretrained:\n        assert W_e is not None\n        \n        X = []\n        \n        wordlist = vocab.get_itos()\n        E = W_e[0].shape[0]\n        B = batch_size\n        \n        # Convert relative vocab to grand_vocab\n        grand_vocab_id = [] # Shape: (V,)\n        for word in wordlist:\n            grand_vocab_id.append(grand_vocab[word])\n        \n        # Extract pretrained embeddings\n        Emb = W_e[grand_vocab_id] # Shape: (V, E)\n        X.append(Emb)\n        \n        # Document one-hot\n        Doc_Emb = torch.zeros(B, E)\n        Doc_Emb[0:B, 0:B] = 1\n        X.append(Doc_Emb.to(DEVICE))\n        \n        # Concat Word embeddings and Doc (one-hot) embeddings\n        X = torch.cat(X, dim= 0) # Shape: (V + B, E)\n        \n        return X\n    \n    else:\n        B = batch_size\n        \n        # Relative vocab is used as the main vocab in training phase. \n        # Given that no pretrained embedding is given.\n        wordlist = vocab.get_itos()\n        V = len(wordlist)\n        \n        # Number of nodes\n        N = B + V\n        \n        X_dict = defaultdict(ret_zero) # One-hot feature matrix X\n        for i in range(N):\n            X_dict[(i, i)] = 1\n        \n        X = build_sparse(X_dict, shape= (N, N))\n        \n        return X\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.812221Z","iopub.execute_input":"2024-06-03T01:26:06.812522Z","iopub.status.idle":"2024-06-03T01:26:06.832729Z","shell.execute_reply.started":"2024-06-03T01:26:06.812497Z","shell.execute_reply":"2024-06-03T01:26:06.831437Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# PyTorch Dataset class\nIn this section, the `NLPDataset` class is defined with custom batch function `collate_batch`.","metadata":{}},{"cell_type":"code","source":"# YELP Dataset\ndef collate_graph(batch):\n    '''\n    Collate batch and building text graph\n    '''\n    raw_texts = []\n    labels = []\n    mask = []\n    texts = []\n    for _text, _label in batch:\n        raw_texts.append(_text)\n        labels.append(_label)\n        \n    vocabulary = get_vocab(raw_texts, unk= False)\n    \n    for _text in raw_texts:\n        texts.append(np.array(text_pipeline(_text, vocabulary)))\n        \n    return texts, labels, vocabulary\n\nclass NLPDataset(Dataset):\n    def __init__(self, df, df_sort= False):\n        self.df = df\n        if df_sort:\n            self.sort_df_by_txt_len()\n    \n    def sort_df_by_txt_len(self):\n        len_list = [-len(self.df.iloc[i]['text']) for i in range(len(self.df))]\n        self.df = self.df.iloc[np.argsort(len_list)]\n    \n    def __len__(self):\n        return len(self.df)\n\n    def label_pipeline(self, x):\n        return int(x)\n    \n    def __getitem__(self, idx):\n        txt = self.df.iloc[idx]['text']\n        \n        label = self.label_pipeline(self.df.iloc[idx]['label'])\n        \n        return (txt, label)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.834387Z","iopub.execute_input":"2024-06-03T01:26:06.834837Z","iopub.status.idle":"2024-06-03T01:26:06.847861Z","shell.execute_reply.started":"2024-06-03T01:26:06.834773Z","shell.execute_reply":"2024-06-03T01:26:06.846800Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Dataset, Dataloader\ntrainset = NLPDataset(train_df)\nvalset = NLPDataset(val_df)\ntestset = NLPDataset(test_df)\nfullset = NLPDataset(df)\n\ntrainloader = DataLoader(trainset, batch_size= len(trainset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\nvalloader = DataLoader(valset, batch_size= len(valset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\ntestloader = DataLoader(testset, batch_size= len(testset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\nfullloader = DataLoader(fullset, batch_size= len(fullset), \n                         shuffle= True, pin_memory= True, collate_fn= collate_graph)\n\nfor batch in fullloader:\n    X, y, vocabulary = batch\n    print(\"Shape of Texts:\", len(X), len(X[0]))\n    print(\"Shape of Labels:\", len(y))\n    GLOBAL_V = len(vocabulary)\n    print(\"Length of Vocab:\", len(vocabulary))\n    GLOBAL_N = len(vocabulary) + len(y) # ONLY ONE BIG GRAPH IS NEEDED FOR TRAINING !\n    print(\"Number of Nodes:\", GLOBAL_N)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:06.849455Z","iopub.execute_input":"2024-06-03T01:26:06.850414Z","iopub.status.idle":"2024-06-03T01:26:08.621561Z","shell.execute_reply.started":"2024-06-03T01:26:06.850375Z","shell.execute_reply":"2024-06-03T01:26:08.620264Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Shape of Texts: 10662 23\nShape of Labels: 10662\nLength of Vocab: 18340\nNumber of Nodes: 29002\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TextGCN Module","metadata":{}},{"cell_type":"code","source":"# TextGCN Module\nclass TextGCN(nn.Module):\n    def __init__(self, graph_emb_size, num_classes, dropout= 0.5, X_emb_size= None, sparse= True):\n        super(TextGCN, self).__init__()\n        self.sparse = sparse\n        self.E = graph_emb_size\n        \n        if not sparse:\n            self.layer = nn.Sequential(\n                nn.LazyLinear(self.E), # Lazy linear for flexible initial embedding\n                nn.Dropout(p= dropout),\n                nn.ReLU(),\n            )\n        else:\n            assert X_emb_size is not None\n            self.N = X_emb_size\n            self.W0 = nn.Parameter(torch.randn(int(X_emb_size), graph_emb_size))\n            torch.nn.init.xavier_uniform_(self.W0.data)\n            self.b0 = nn.Parameter(torch.zeros((int(X_emb_size), graph_emb_size)))\n#             torch.nn.init.xavier_uniform_(self.b0.data)\n            self.afterlinear = nn.Sequential(\n                nn.ReLU(),\n                nn.Dropout(p= dropout)\n            )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(self.E, num_classes),\n            nn.Softmax(dim= 1)\n        )\n    \n    def forward(self, V, G, X, check_shape= False):\n        '''\n        <G> : Graph matrix, expected to have shape (N, N) and symetric\n        <X> : Initial embedding. Expected to have shape (N, E_0)\n        \n        NOTES: If 'sparse' is True, G, X are given in scipy.sparse.csr_matrix format\n        \n        Vocab size: V\n        Batch size: B\n        Nodes: N = B + V\n        Graph embedding size: E\n        Initial embedding size: E_0\n        '''\n        # First layer:\n        if not self.sparse:\n            L1 = self.layer(torch.matmul(G, X)) # Shape: (N, E_0)\n        else:\n            L1 = G * X # Shape: (N, N), type = sp.csr_matrix\n            L1 = csr2spMat(L1).to(DEVICE)\n            L1 = torch.sparse.addmm(self.b0, L1, self.W0) # Shape: (N, E), type = dense matrix\n            L1 = self.afterlinear(L1)\n        \n        # Classifier:\n        if not self.sparse:\n            logits = self.classifier(torch.matmul(G, L1)) # Shape: (N, num_classes)\n        else:\n            newG = csr2spMat(G).to(DEVICE)\n            logits = self.classifier(torch.sparse.mm(newG, L1).to(DEVICE)).to(DEVICE) # Shape: (N, num_classes)\n        \n        doc_logits = logits[V:, :] # Shape: (B, num_classes)\n        \n        if check_shape:\n            print('*********************************')\n            print('Shape of L1:', L1.shape)\n            print('Shape of logits:', logits.shape)\n            print('Shape of doc_logits:', doc_logits.shape)\n        \n        return logits\n    \ndef init_weights(m):\n    '''\n    Weight initialization.\n    '''\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:08.623022Z","iopub.execute_input":"2024-06-03T01:26:08.623462Z","iopub.status.idle":"2024-06-03T01:26:08.637859Z","shell.execute_reply.started":"2024-06-03T01:26:08.623423Z","shell.execute_reply":"2024-06-03T01:26:08.636616Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Model initialization\nmodel = TextGCN(EMBEDDING_SIZE, NUM_CLASSES, DROPOUT, GLOBAL_N).to(DEVICE)\nmodel.apply(init_weights)\n# for name, param in model.named_parameters():\n#     print(name) \n#     print(param.is_leaf)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:31:50.587033Z","iopub.execute_input":"2024-06-03T01:31:50.587439Z","iopub.status.idle":"2024-06-03T01:31:50.695280Z","shell.execute_reply.started":"2024-06-03T01:31:50.587406Z","shell.execute_reply":"2024-06-03T01:31:50.694128Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"TextGCN(\n  (afterlinear): Sequential(\n    (0): ReLU()\n    (1): Dropout(p=0.5, inplace=False)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=200, out_features=2, bias=True)\n    (1): Softmax(dim=1)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training\n\n## Prepare text graphs\nGenerating text graphs from text corpus is a considerable computational bottleneck. Hence, the graph $G$ and input $X$ should be calculated in advance. Following the original paper, $X$ is constructed using one-hot encoding over vocabulary and documents inferred from the corpus. Hence, $X\\in\\mathbb{R}^{N\\times N}$ where $N = B + V$, with $V$ being vocabulary size and $B$ being number of documents.\n\nHence, during inference time, texts must be load in a batch of size $B$ and tokenize using the training vocabulary in order to maintain dimensional consistency with pretrained **TextGCN** model.","metadata":{}},{"cell_type":"code","source":"# Preparing text graphs\n\n# Train graphs\nG_trainlist = []\nX_trainlist = []\ny_trainlist = []\n\nfor i, (texts, labels, vocabulary) in enumerate(fullloader):\n    # Build graph\n    G = build_graph(texts, vocabulary, WINDOW_SIZE) # Graph matrix\n    N = G.shape[0] # Number of nodes\n    \n    print('Number of nodes: N = ', N)\n    torch.save(vocabulary, 'train_vocab.pth')\n\n    # Input features.\n    # X = build_X(vocabulary, grand_vocab, len(labels), W_e= W_e) # Pretrained embedding mode. Not efficient.\n    # One-hot encoding mode\n    X_dict = defaultdict(ret_zero)\n    for i in range(N):\n        X_dict[(i, i)] = 1\n    X = build_sparse(X_dict, shape= (N, N))\n    y = torch.tensor(labels)\n    \n    G_trainlist.append(G)\n    X_trainlist.append(X)\n    y_trainlist.append(y)\n    \n    break # Only ONE BIG GRAPH is needed for training\n\nGXy_train = (G_trainlist, X_trainlist, y_trainlist)\ntorch.save(GXy_train, \"GXy.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:26:08.786925Z","iopub.execute_input":"2024-06-03T01:26:08.787213Z","iopub.status.idle":"2024-06-03T01:27:21.201491Z","shell.execute_reply.started":"2024-06-03T01:26:08.787190Z","shell.execute_reply":"2024-06-03T01:27:21.200150Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Number of nodes: N =  29002\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train-Val-Test idx's\ntrain_idx = list(range(0,len(train_df)))\nval_idx = list(range(len(train_df), len(train_df) + len(val_df)))\ntest_idx = list(range(len(train_df) + len(val_df), len(df)))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:27:21.202864Z","iopub.execute_input":"2024-06-03T01:27:21.203260Z","iopub.status.idle":"2024-06-03T01:27:21.209294Z","shell.execute_reply.started":"2024-06-03T01:27:21.203229Z","shell.execute_reply":"2024-06-03T01:27:21.208092Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Load prebuilt graph if available\n# GXy_train = torch.load('/kaggle/input/textgcn-exps/GXy.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:27:21.210537Z","iopub.execute_input":"2024-06-03T01:27:21.210966Z","iopub.status.idle":"2024-06-03T01:27:21.222154Z","shell.execute_reply.started":"2024-06-03T01:27:21.210937Z","shell.execute_reply":"2024-06-03T01:27:21.221024Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Training configs","metadata":{}},{"cell_type":"code","source":"# Training configs\nLR = 0.02\n\nEPOCHS = 200\nITER = EPOCHS\n\nOPTIMIZER = torch.optim.Adam(model.parameters(), lr= LR)\n# OPTIMIZER = torch.optim.SGD(model.parameters(), lr= LR, momentum= 0.9, nesterov= True)\n\n# SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max = ITER)\nSCHEDULER = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(OPTIMIZER, T_0= 25, T_mult = 1, eta_min= 0)\n\nLOSS_FN = nn.CrossEntropyLoss()\nRECORD = 0.0","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:31:59.992942Z","iopub.execute_input":"2024-06-03T01:31:59.993368Z","iopub.status.idle":"2024-06-03T01:32:00.000869Z","shell.execute_reply.started":"2024-06-03T01:31:59.993337Z","shell.execute_reply":"2024-06-03T01:31:59.999397Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Training procedures","metadata":{}},{"cell_type":"code","source":"# Train procedures\ndef test(GXy, test_idx, model, loss_fn):\n    global GLOBAL_V\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    G_list, X_list, y_list = GXy\n    L = len(G_list)\n    n_samples = 0\n    \n    for i in range(L):\n        G, X, y = G_list[i], X_list[i], y_list[i][test_idx]\n        y = y.to(DEVICE)\n        batch_size = y.shape[0]\n        N = G.shape[0]\n        \n        rel_idx = [x + GLOBAL_V for x in test_idx]\n        \n        # Get loss and inference\n        logits = model(GLOBAL_V, G, X)\n        logits = logits[rel_idx, :]\n        loss = loss_fn(logits, y)\n        n_samples += (batch_size)\n        correct += (logits.argmax(dim= 1) == y).type(torch.float).sum().item()\n\n        test_loss += loss\n        \n    test_loss /= L\n    accuracy = correct / n_samples\n    \n    return test_loss, accuracy\n\ndef train(GXy_train, train_idx, model, optimizer, scheduler, loss_fn, val_freq):\n    global RECORD, GLOBAL_V\n    model.train()\n    tloss = []\n    \n    G_list, X_list, y_list = GXy_train\n    L = len(G_list)\n    \n    for i in range(L):\n        G, X, y = G_list[i], X_list[i], y_list[i][train_idx]\n        y = y.to(DEVICE)\n        \n        rel_idx = [x + GLOBAL_V for x in train_idx]\n        \n        # Get loss\n        logits = model(GLOBAL_V, G, X)\n        logits = logits[rel_idx, :]\n        loss = loss_fn(logits, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n#         scheduler.step()\n        \n        tloss.append(loss.cpu().detach().numpy())\n        \n    return tloss","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:27:23.449501Z","iopub.execute_input":"2024-06-03T01:27:23.450094Z","iopub.status.idle":"2024-06-03T01:27:23.463274Z","shell.execute_reply.started":"2024-06-03T01:27:23.450063Z","shell.execute_reply":"2024-06-03T01:27:23.462176Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# TRAINING \niter_loss = []\nepoch_loss = []\nbest_acc = 0\n\nwith torch.no_grad():\n    pass\n\npatience = 10\nprev_val_loss = 1e9 \nacc_record = 0\n\nfor t in range(EPOCHS):\n    tloss_train = train(GXy_train, train_idx, model, OPTIMIZER, SCHEDULER, LOSS_FN, 10)\n    \n    iter_loss = iter_loss + tloss_train  \n    epoch_loss.append(sum(tloss_train) / len(tloss_train))\n    \n    val_loss, val_acc = test(GXy_train, val_idx, model, LOSS_FN)\n    print(f'Epoch {t}: LOSS = {epoch_loss[-1]}, val-loss = {val_loss:.3f}, val-acc = {val_acc:.3f}')\n    \n#     if val_loss >= prev_val_loss:\n#         patience -= 1\n#         if patience == 0:\n#             break\n    \n#     else:\n#         patience = 10\n        \n    if val_acc > acc_record:\n        acc_record = val_acc\n        torch.save(model.state_dict(), f'TextGCN_best.pth')\n        \n    prev_val_loss = val_loss\n    \ntorch.save(model.state_dict(), f'TextGCN_last.pth')\n    \nfig, axes = plt.subplots()\naxes.plot(epoch_loss, label = 'train-loss')\naxes.legend()\naxes.set_xlabel('Iteration')\naxes.set_ylabel('Loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:32:04.781453Z","iopub.execute_input":"2024-06-03T01:32:04.781908Z","iopub.status.idle":"2024-06-03T01:37:11.112527Z","shell.execute_reply.started":"2024-06-03T01:32:04.781875Z","shell.execute_reply":"2024-06-03T01:37:11.111369Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 0: LOSS = 0.6931651830673218, val-loss = 0.692, val-acc = 0.501\nEpoch 1: LOSS = 0.6877940893173218, val-loss = 0.690, val-acc = 0.502\nEpoch 2: LOSS = 0.6766778230667114, val-loss = 0.686, val-acc = 0.614\nEpoch 3: LOSS = 0.6592056155204773, val-loss = 0.681, val-acc = 0.726\nEpoch 4: LOSS = 0.6350416541099548, val-loss = 0.675, val-acc = 0.751\nEpoch 5: LOSS = 0.6046751737594604, val-loss = 0.668, val-acc = 0.758\nEpoch 6: LOSS = 0.569318950176239, val-loss = 0.660, val-acc = 0.757\nEpoch 7: LOSS = 0.531037449836731, val-loss = 0.650, val-acc = 0.757\nEpoch 8: LOSS = 0.4919259250164032, val-loss = 0.640, val-acc = 0.758\nEpoch 9: LOSS = 0.45402991771698, val-loss = 0.630, val-acc = 0.757\nEpoch 10: LOSS = 0.4201294183731079, val-loss = 0.619, val-acc = 0.760\nEpoch 11: LOSS = 0.39171043038368225, val-loss = 0.609, val-acc = 0.760\nEpoch 12: LOSS = 0.3692206144332886, val-loss = 0.600, val-acc = 0.760\nEpoch 13: LOSS = 0.3524104058742523, val-loss = 0.592, val-acc = 0.758\nEpoch 14: LOSS = 0.3402256369590759, val-loss = 0.584, val-acc = 0.759\nEpoch 15: LOSS = 0.3316212296485901, val-loss = 0.578, val-acc = 0.758\nEpoch 16: LOSS = 0.32573094964027405, val-loss = 0.573, val-acc = 0.758\nEpoch 17: LOSS = 0.3217773735523224, val-loss = 0.569, val-acc = 0.758\nEpoch 18: LOSS = 0.31903374195098877, val-loss = 0.565, val-acc = 0.759\nEpoch 19: LOSS = 0.31732481718063354, val-loss = 0.562, val-acc = 0.759\nEpoch 20: LOSS = 0.3161678612232208, val-loss = 0.560, val-acc = 0.758\nEpoch 21: LOSS = 0.3153107762336731, val-loss = 0.558, val-acc = 0.756\nEpoch 22: LOSS = 0.31480199098587036, val-loss = 0.556, val-acc = 0.757\nEpoch 23: LOSS = 0.3143817186355591, val-loss = 0.555, val-acc = 0.757\nEpoch 24: LOSS = 0.3140832185745239, val-loss = 0.553, val-acc = 0.756\nEpoch 25: LOSS = 0.3139016330242157, val-loss = 0.552, val-acc = 0.756\nEpoch 26: LOSS = 0.3137752115726471, val-loss = 0.552, val-acc = 0.756\nEpoch 27: LOSS = 0.3136596977710724, val-loss = 0.551, val-acc = 0.756\nEpoch 28: LOSS = 0.3135725259780884, val-loss = 0.550, val-acc = 0.756\nEpoch 29: LOSS = 0.31351926922798157, val-loss = 0.550, val-acc = 0.756\nEpoch 30: LOSS = 0.3134726285934448, val-loss = 0.549, val-acc = 0.756\nEpoch 31: LOSS = 0.31343507766723633, val-loss = 0.549, val-acc = 0.755\nEpoch 32: LOSS = 0.3134075105190277, val-loss = 0.549, val-acc = 0.755\nEpoch 33: LOSS = 0.31338900327682495, val-loss = 0.549, val-acc = 0.755\nEpoch 34: LOSS = 0.3133727014064789, val-loss = 0.548, val-acc = 0.755\nEpoch 35: LOSS = 0.31336572766304016, val-loss = 0.548, val-acc = 0.756\nEpoch 36: LOSS = 0.313347727060318, val-loss = 0.548, val-acc = 0.757\nEpoch 37: LOSS = 0.31333911418914795, val-loss = 0.548, val-acc = 0.757\nEpoch 38: LOSS = 0.3133334219455719, val-loss = 0.548, val-acc = 0.757\nEpoch 39: LOSS = 0.31332552433013916, val-loss = 0.547, val-acc = 0.757\nEpoch 40: LOSS = 0.31331944465637207, val-loss = 0.547, val-acc = 0.757\nEpoch 41: LOSS = 0.31331855058670044, val-loss = 0.547, val-acc = 0.758\nEpoch 42: LOSS = 0.31331250071525574, val-loss = 0.547, val-acc = 0.759\nEpoch 43: LOSS = 0.3133094012737274, val-loss = 0.547, val-acc = 0.759\nEpoch 44: LOSS = 0.3133060038089752, val-loss = 0.547, val-acc = 0.759\nEpoch 45: LOSS = 0.31330475211143494, val-loss = 0.547, val-acc = 0.759\nEpoch 46: LOSS = 0.31330135464668274, val-loss = 0.547, val-acc = 0.759\nEpoch 47: LOSS = 0.31330251693725586, val-loss = 0.547, val-acc = 0.759\nEpoch 48: LOSS = 0.3132970631122589, val-loss = 0.547, val-acc = 0.759\nEpoch 49: LOSS = 0.31329816579818726, val-loss = 0.547, val-acc = 0.759\nEpoch 50: LOSS = 0.3132964074611664, val-loss = 0.546, val-acc = 0.758\nEpoch 51: LOSS = 0.3132942020893097, val-loss = 0.546, val-acc = 0.758\nEpoch 52: LOSS = 0.3132932782173157, val-loss = 0.546, val-acc = 0.758\nEpoch 53: LOSS = 0.31329286098480225, val-loss = 0.546, val-acc = 0.758\nEpoch 54: LOSS = 0.3132907450199127, val-loss = 0.546, val-acc = 0.758\nEpoch 55: LOSS = 0.3132913410663605, val-loss = 0.546, val-acc = 0.756\nEpoch 56: LOSS = 0.31329041719436646, val-loss = 0.546, val-acc = 0.756\nEpoch 57: LOSS = 0.3132892847061157, val-loss = 0.546, val-acc = 0.756\nEpoch 58: LOSS = 0.3132897615432739, val-loss = 0.546, val-acc = 0.755\nEpoch 59: LOSS = 0.3132895231246948, val-loss = 0.546, val-acc = 0.755\nEpoch 60: LOSS = 0.31328919529914856, val-loss = 0.546, val-acc = 0.755\nEpoch 61: LOSS = 0.3132881224155426, val-loss = 0.546, val-acc = 0.755\nEpoch 62: LOSS = 0.3132879436016083, val-loss = 0.546, val-acc = 0.755\nEpoch 63: LOSS = 0.31328797340393066, val-loss = 0.546, val-acc = 0.755\nEpoch 64: LOSS = 0.3132867217063904, val-loss = 0.546, val-acc = 0.755\nEpoch 65: LOSS = 0.3132866621017456, val-loss = 0.546, val-acc = 0.755\nEpoch 66: LOSS = 0.313286155462265, val-loss = 0.546, val-acc = 0.755\nEpoch 67: LOSS = 0.31328627467155457, val-loss = 0.546, val-acc = 0.755\nEpoch 68: LOSS = 0.31328654289245605, val-loss = 0.546, val-acc = 0.755\nEpoch 69: LOSS = 0.3132866621017456, val-loss = 0.546, val-acc = 0.755\nEpoch 70: LOSS = 0.3132856488227844, val-loss = 0.546, val-acc = 0.755\nEpoch 71: LOSS = 0.31328627467155457, val-loss = 0.546, val-acc = 0.755\nEpoch 72: LOSS = 0.3132856786251068, val-loss = 0.546, val-acc = 0.755\nEpoch 73: LOSS = 0.31328606605529785, val-loss = 0.546, val-acc = 0.755\nEpoch 74: LOSS = 0.3132852017879486, val-loss = 0.546, val-acc = 0.755\nEpoch 75: LOSS = 0.3132842481136322, val-loss = 0.546, val-acc = 0.755\nEpoch 76: LOSS = 0.31328529119491577, val-loss = 0.546, val-acc = 0.755\nEpoch 77: LOSS = 0.3132849931716919, val-loss = 0.546, val-acc = 0.755\nEpoch 78: LOSS = 0.31328511238098145, val-loss = 0.546, val-acc = 0.755\nEpoch 79: LOSS = 0.3132851719856262, val-loss = 0.546, val-acc = 0.755\nEpoch 80: LOSS = 0.3132849335670471, val-loss = 0.546, val-acc = 0.755\nEpoch 81: LOSS = 0.313283771276474, val-loss = 0.546, val-acc = 0.755\nEpoch 82: LOSS = 0.31328466534614563, val-loss = 0.546, val-acc = 0.755\nEpoch 83: LOSS = 0.31328460574150085, val-loss = 0.546, val-acc = 0.755\nEpoch 84: LOSS = 0.3132854998111725, val-loss = 0.546, val-acc = 0.756\nEpoch 85: LOSS = 0.313284307718277, val-loss = 0.546, val-acc = 0.756\nEpoch 86: LOSS = 0.31328433752059937, val-loss = 0.546, val-acc = 0.756\nEpoch 87: LOSS = 0.31328463554382324, val-loss = 0.546, val-acc = 0.756\nEpoch 88: LOSS = 0.313284695148468, val-loss = 0.546, val-acc = 0.756\nEpoch 89: LOSS = 0.3132842183113098, val-loss = 0.545, val-acc = 0.756\nEpoch 90: LOSS = 0.3132841885089874, val-loss = 0.545, val-acc = 0.756\nEpoch 91: LOSS = 0.31328433752059937, val-loss = 0.545, val-acc = 0.756\nEpoch 92: LOSS = 0.31328439712524414, val-loss = 0.545, val-acc = 0.755\nEpoch 93: LOSS = 0.3132840096950531, val-loss = 0.545, val-acc = 0.754\nEpoch 94: LOSS = 0.3132838308811188, val-loss = 0.545, val-acc = 0.754\nEpoch 95: LOSS = 0.31328293681144714, val-loss = 0.545, val-acc = 0.754\nEpoch 96: LOSS = 0.3132835328578949, val-loss = 0.545, val-acc = 0.754\nEpoch 97: LOSS = 0.31328436732292175, val-loss = 0.545, val-acc = 0.754\nEpoch 98: LOSS = 0.31328335404396057, val-loss = 0.545, val-acc = 0.754\nEpoch 99: LOSS = 0.3132835030555725, val-loss = 0.545, val-acc = 0.754\nEpoch 100: LOSS = 0.31328409910202026, val-loss = 0.545, val-acc = 0.754\nEpoch 101: LOSS = 0.3132837116718292, val-loss = 0.545, val-acc = 0.754\nEpoch 102: LOSS = 0.3132833242416382, val-loss = 0.545, val-acc = 0.752\nEpoch 103: LOSS = 0.31328338384628296, val-loss = 0.545, val-acc = 0.752\nEpoch 104: LOSS = 0.31328284740448, val-loss = 0.545, val-acc = 0.752\nEpoch 105: LOSS = 0.3132828176021576, val-loss = 0.545, val-acc = 0.752\nEpoch 106: LOSS = 0.3132832646369934, val-loss = 0.545, val-acc = 0.752\nEpoch 107: LOSS = 0.3132832944393158, val-loss = 0.545, val-acc = 0.752\nEpoch 108: LOSS = 0.31328243017196655, val-loss = 0.545, val-acc = 0.752\nEpoch 109: LOSS = 0.3132832944393158, val-loss = 0.545, val-acc = 0.752\nEpoch 110: LOSS = 0.3132832646369934, val-loss = 0.545, val-acc = 0.752\nEpoch 111: LOSS = 0.31328293681144714, val-loss = 0.545, val-acc = 0.752\nEpoch 112: LOSS = 0.3132830262184143, val-loss = 0.545, val-acc = 0.752\nEpoch 113: LOSS = 0.31328290700912476, val-loss = 0.545, val-acc = 0.752\nEpoch 114: LOSS = 0.31328290700912476, val-loss = 0.545, val-acc = 0.752\nEpoch 115: LOSS = 0.3132829964160919, val-loss = 0.545, val-acc = 0.752\nEpoch 116: LOSS = 0.3132830858230591, val-loss = 0.545, val-acc = 0.752\nEpoch 117: LOSS = 0.3132832944393158, val-loss = 0.545, val-acc = 0.752\nEpoch 118: LOSS = 0.3132830858230591, val-loss = 0.545, val-acc = 0.752\nEpoch 119: LOSS = 0.3132828176021576, val-loss = 0.545, val-acc = 0.752\nEpoch 120: LOSS = 0.3132827579975128, val-loss = 0.545, val-acc = 0.752\nEpoch 121: LOSS = 0.31328344345092773, val-loss = 0.545, val-acc = 0.752\nEpoch 122: LOSS = 0.31328240036964417, val-loss = 0.545, val-acc = 0.752\nEpoch 123: LOSS = 0.3132830858230591, val-loss = 0.545, val-acc = 0.752\nEpoch 124: LOSS = 0.3132822811603546, val-loss = 0.545, val-acc = 0.753\nEpoch 125: LOSS = 0.3132825791835785, val-loss = 0.545, val-acc = 0.753\nEpoch 126: LOSS = 0.3132828176021576, val-loss = 0.545, val-acc = 0.753\nEpoch 127: LOSS = 0.31328171491622925, val-loss = 0.545, val-acc = 0.753\nEpoch 128: LOSS = 0.313282310962677, val-loss = 0.545, val-acc = 0.753\nEpoch 129: LOSS = 0.3132822811603546, val-loss = 0.545, val-acc = 0.753\nEpoch 130: LOSS = 0.3132821321487427, val-loss = 0.545, val-acc = 0.753\nEpoch 131: LOSS = 0.3132820725440979, val-loss = 0.545, val-acc = 0.753\nEpoch 132: LOSS = 0.3132821023464203, val-loss = 0.545, val-acc = 0.753\nEpoch 133: LOSS = 0.31328245997428894, val-loss = 0.545, val-acc = 0.753\nEpoch 134: LOSS = 0.31328248977661133, val-loss = 0.545, val-acc = 0.753\nEpoch 135: LOSS = 0.3132818043231964, val-loss = 0.545, val-acc = 0.754\nEpoch 136: LOSS = 0.3132820129394531, val-loss = 0.545, val-acc = 0.754\nEpoch 137: LOSS = 0.31328144669532776, val-loss = 0.545, val-acc = 0.754\nEpoch 138: LOSS = 0.313282310962677, val-loss = 0.545, val-acc = 0.754\nEpoch 139: LOSS = 0.31328222155570984, val-loss = 0.545, val-acc = 0.753\nEpoch 140: LOSS = 0.31328222155570984, val-loss = 0.545, val-acc = 0.753\nEpoch 141: LOSS = 0.31328144669532776, val-loss = 0.545, val-acc = 0.753\nEpoch 142: LOSS = 0.3132811188697815, val-loss = 0.545, val-acc = 0.753\nEpoch 143: LOSS = 0.3132820129394531, val-loss = 0.545, val-acc = 0.753\nEpoch 144: LOSS = 0.3132816553115845, val-loss = 0.545, val-acc = 0.753\nEpoch 145: LOSS = 0.31328171491622925, val-loss = 0.545, val-acc = 0.753\nEpoch 146: LOSS = 0.31328123807907104, val-loss = 0.545, val-acc = 0.753\nEpoch 147: LOSS = 0.3132820427417755, val-loss = 0.545, val-acc = 0.753\nEpoch 148: LOSS = 0.31328168511390686, val-loss = 0.545, val-acc = 0.753\nEpoch 149: LOSS = 0.3132815957069397, val-loss = 0.545, val-acc = 0.753\nEpoch 150: LOSS = 0.31328222155570984, val-loss = 0.545, val-acc = 0.753\nEpoch 151: LOSS = 0.31328126788139343, val-loss = 0.545, val-acc = 0.753\nEpoch 152: LOSS = 0.3132812976837158, val-loss = 0.545, val-acc = 0.753\nEpoch 153: LOSS = 0.31328144669532776, val-loss = 0.545, val-acc = 0.753\nEpoch 154: LOSS = 0.3132815957069397, val-loss = 0.545, val-acc = 0.753\nEpoch 155: LOSS = 0.3132806122303009, val-loss = 0.545, val-acc = 0.753\nEpoch 156: LOSS = 0.31328174471855164, val-loss = 0.545, val-acc = 0.753\nEpoch 157: LOSS = 0.31328096985816956, val-loss = 0.545, val-acc = 0.753\nEpoch 158: LOSS = 0.31328174471855164, val-loss = 0.545, val-acc = 0.753\nEpoch 159: LOSS = 0.3132812976837158, val-loss = 0.545, val-acc = 0.753\nEpoch 160: LOSS = 0.31328046321868896, val-loss = 0.545, val-acc = 0.753\nEpoch 161: LOSS = 0.31328099966049194, val-loss = 0.545, val-acc = 0.753\nEpoch 162: LOSS = 0.3132808208465576, val-loss = 0.545, val-acc = 0.753\nEpoch 163: LOSS = 0.31328028440475464, val-loss = 0.545, val-acc = 0.752\nEpoch 164: LOSS = 0.31328076124191284, val-loss = 0.545, val-acc = 0.752\nEpoch 165: LOSS = 0.313280314207077, val-loss = 0.545, val-acc = 0.752\nEpoch 166: LOSS = 0.3132803738117218, val-loss = 0.545, val-acc = 0.752\nEpoch 167: LOSS = 0.31328049302101135, val-loss = 0.544, val-acc = 0.751\nEpoch 168: LOSS = 0.31328073143959045, val-loss = 0.544, val-acc = 0.751\nEpoch 169: LOSS = 0.3132803440093994, val-loss = 0.544, val-acc = 0.751\nEpoch 170: LOSS = 0.31328001618385315, val-loss = 0.544, val-acc = 0.751\nEpoch 171: LOSS = 0.3132795989513397, val-loss = 0.544, val-acc = 0.751\nEpoch 172: LOSS = 0.31328070163726807, val-loss = 0.544, val-acc = 0.751\nEpoch 173: LOSS = 0.3132804334163666, val-loss = 0.544, val-acc = 0.751\nEpoch 174: LOSS = 0.3132801949977875, val-loss = 0.544, val-acc = 0.751\nEpoch 175: LOSS = 0.31327998638153076, val-loss = 0.544, val-acc = 0.751\nEpoch 176: LOSS = 0.3132797181606293, val-loss = 0.544, val-acc = 0.751\nEpoch 177: LOSS = 0.31328022480010986, val-loss = 0.544, val-acc = 0.751\nEpoch 178: LOSS = 0.31327977776527405, val-loss = 0.544, val-acc = 0.751\nEpoch 179: LOSS = 0.31327977776527405, val-loss = 0.544, val-acc = 0.751\nEpoch 180: LOSS = 0.3132801949977875, val-loss = 0.544, val-acc = 0.751\nEpoch 181: LOSS = 0.31328070163726807, val-loss = 0.544, val-acc = 0.751\nEpoch 182: LOSS = 0.3132798671722412, val-loss = 0.544, val-acc = 0.751\nEpoch 183: LOSS = 0.3132796883583069, val-loss = 0.544, val-acc = 0.751\nEpoch 184: LOSS = 0.3132801651954651, val-loss = 0.544, val-acc = 0.751\nEpoch 185: LOSS = 0.3132796883583069, val-loss = 0.544, val-acc = 0.751\nEpoch 186: LOSS = 0.3132796883583069, val-loss = 0.544, val-acc = 0.751\nEpoch 187: LOSS = 0.3132796585559845, val-loss = 0.544, val-acc = 0.751\nEpoch 188: LOSS = 0.31327998638153076, val-loss = 0.544, val-acc = 0.751\nEpoch 189: LOSS = 0.313279926776886, val-loss = 0.544, val-acc = 0.751\nEpoch 190: LOSS = 0.31328079104423523, val-loss = 0.544, val-acc = 0.751\nEpoch 191: LOSS = 0.31327930092811584, val-loss = 0.544, val-acc = 0.751\nEpoch 192: LOSS = 0.31327903270721436, val-loss = 0.544, val-acc = 0.752\nEpoch 193: LOSS = 0.3132794499397278, val-loss = 0.544, val-acc = 0.752\nEpoch 194: LOSS = 0.3132789134979248, val-loss = 0.544, val-acc = 0.752\nEpoch 195: LOSS = 0.31327909231185913, val-loss = 0.544, val-acc = 0.752\nEpoch 196: LOSS = 0.31327956914901733, val-loss = 0.544, val-acc = 0.752\nEpoch 197: LOSS = 0.31327950954437256, val-loss = 0.544, val-acc = 0.752\nEpoch 198: LOSS = 0.3132798671722412, val-loss = 0.544, val-acc = 0.752\nEpoch 199: LOSS = 0.3132791817188263, val-loss = 0.544, val-acc = 0.752\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"inputs = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\nloss = LOSS_FN(inputs, target)\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:37:39.818869Z","iopub.execute_input":"2024-06-03T01:37:39.819281Z","iopub.status.idle":"2024-06-03T01:37:39.837796Z","shell.execute_reply.started":"2024-06-03T01:37:39.819250Z","shell.execute_reply":"2024-06-03T01:37:39.836631Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"tensor(2.9694, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Verify training accuracy","metadata":{}},{"cell_type":"code","source":"# GXy_train[0][0].to(CPU), GXy_train[1][0].to(CPU), GXy_train[2][0].to(CPU)\nprint(f'LAST ACCURACY')\n_, test_acc = test(GXy_train, train_idx, model, LOSS_FN)\nprint(f'Train accuracy: {test_acc}')\n\n_, test_acc = test(GXy_train, val_idx, model, LOSS_FN)\nprint(f'Val accuracy: {test_acc}')\n\n_, test_acc = test(GXy_train, test_idx, model, LOSS_FN)\nprint(f'Test accuracy: {test_acc}')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:37:41.935626Z","iopub.execute_input":"2024-06-03T01:37:41.936368Z","iopub.status.idle":"2024-06-03T01:37:43.491464Z","shell.execute_reply.started":"2024-06-03T01:37:41.936333Z","shell.execute_reply":"2024-06-03T01:37:43.490408Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"LAST ACCURACY\nTrain accuracy: 1.0\nVal accuracy: 0.7523452157598499\nTest accuracy: 0.7497656982193065\n","output_type":"stream"}]},{"cell_type":"code","source":"# GXy_train[0][0].to(CPU), GXy_train[1][0].to(CPU), GXy_train[2][0].to(CPU)\nprint(f'BEST ACCURACY')\nmodel.load_state_dict(torch.load('/kaggle/working/TextGCN_best.pth'))\n_, test_acc = test(GXy_train, train_idx, model, LOSS_FN)\nprint(f'Train accuracy: {test_acc}')\n\n_, test_acc = test(GXy_train, val_idx, model, LOSS_FN)\nprint(f'Val accuracy: {test_acc}')\n\n_, test_acc = test(GXy_train, test_idx, model, LOSS_FN)\nprint(f'Test accuracy: {test_acc}')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T01:37:46.092266Z","iopub.execute_input":"2024-06-03T01:37:46.092663Z","iopub.status.idle":"2024-06-03T01:37:47.650767Z","shell.execute_reply.started":"2024-06-03T01:37:46.092632Z","shell.execute_reply":"2024-06-03T01:37:47.649554Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"BEST ACCURACY\nTrain accuracy: 0.9998827529604878\nVal accuracy: 0.7598499061913696\nTest accuracy: 0.7600749765698219\n","output_type":"stream"}]}]}